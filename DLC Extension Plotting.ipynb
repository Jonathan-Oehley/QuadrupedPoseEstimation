{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DLC Extension Plotting.ipynb","provenance":[{"file_id":"1ERgsp6pTDFpZD6yLHAW-WQB8l1z50Kmx","timestamp":1570922391016}],"collapsed_sections":["snP56FXP0mEZ","oJ0op7aiWEmS","PPPloJxprDdI"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"StEH0RAw0mEm"},"source":["# Setup Code"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k-vf1MaC0mEi"},"source":["### Drive Setup"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"b4a3a2b7-9230-4759-e885-a1133c64e80e","executionInfo":{"status":"ok","timestamp":1571035488476,"user_tz":-120,"elapsed":1198,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"id":"nUiuRcjp0mEb","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"snP56FXP0mEZ"},"source":["### DLC Setup\n","Code created to automatically crash the notebook to reload the dependencies that were imported. Requires manual comment of os.kill line of code after the first run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h4UoOUfI0mET","colab":{}},"source":["# Download and installation\n","%cd /content\n","!git clone -l -s git://github.com/AlexEMG/DeepLabCut.git cloned-DLC-repo\n","%cd cloned-DLC-repo\n","\n","from IPython.display import clear_output\n","# !pip install deeplabcut\n","clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n7Ae0ouo0mEN"},"source":["#### Setup.py write"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"0aa7143e-731e-4c9f-95e4-28ddd224d47b","executionInfo":{"status":"ok","timestamp":1571035491565,"user_tz":-120,"elapsed":4257,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"id":"_zsLk0fY0mEG","colab":{"base_uri":"https://localhost:8080/"}},"source":["%%writefile setup.py\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","DeepLabCut2.0 Toolbox (deeplabcut.org)\n","Â© A. & M. Mathis Labs\n","https://github.com/AlexEMG/DeepLabCut\n","\n","Please see AUTHORS for contributors.\n","https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS\n","Licensed under GNU Lesser General Public License v3.0\n","\"\"\"\n","\n","import setuptools\n","\n","with open(\"README.md\", \"r\") as fh:\n","    long_description = fh.read()\n","\n","setuptools.setup(\n","    name=\"deeplabcut\",\n","    version=\"2.0.9\",\n","    author=\"A. & M. Mathis Labs\",\n","    author_email=\"alexander.mathis@bethgelab.org\",\n","    description=\"Markerless pose-estimation of user-defined features with deep learning\",\n","    long_description=long_description,\n","    long_description_content_type=\"text/markdown\",\n","    url=\"https://github.com/AlexEMG/DeepLabCut\",\n","    install_requires=['certifi','chardet~=3.0.4','click','easydict~=1.7',\n","                      'gast==0.2.2','h5py~=2.7','imageio~=2.3.0','intel-openmp',\n","                      'ipython~=6.0.0','ipython-genutils~=0.2.0',\n","                      'matplotlib~=3.0.3','moviepy~=0.2.3.5','numpy~=1.14.5','opencv-python~=3.4',\n","                      'pandas>=0.21.0','patsy','python-dateutil~=2.7.3','pyyaml>=5.1','requests',\n","                      'ruamel.yaml~=0.15','setuptools','scikit-image~=0.14.0','scikit-learn~=0.19.2',\n","                      'scipy~=1.1.0','statsmodels~=0.9.0','tables',\n","                      'tensorpack~=0.9.7.1',\n","                      'tqdm>4.29','wheel~=0.31.1'],\n","    scripts=['deeplabcut/pose_estimation_tensorflow/models/pretrained/download.sh'],\n","    packages=setuptools.find_packages(),\n","    data_files=[('deeplabcut',['deeplabcut/pose_cfg.yaml','deeplabcut/pose_estimation_tensorflow/models/pretrained/pretrained_model_urls.yaml'])],\n","    include_package_data=True,\n","    classifiers=(\n","        \"Programming Language :: Python :: 3\",\n","        \"License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)\",\n","        \"Operating System :: OS Independent\",\n","    ),\n","    entry_points=\"\"\"[console_scripts]\n","            dlc=dlc:main\"\"\",\n",")\n","\n","#https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting setup.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TsaI-bRJ0mEE"},"source":["### Remaining Setup"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ODkhs7wc0mD4","colab":{}},"source":["%cd /content\n","!pip install -e cloned-DLC-repo\n","clear_output()\n","\n","import os\n","# os.kill(os.getpid(), 9)     # Comment this line out after first run\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HLmZcrZk0mDl","outputId":"d5b66fd1-ca1c-48a0-8e8b-e6b463202e9e","executionInfo":{"status":"ok","timestamp":1571035500795,"user_tz":-120,"elapsed":13453,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Environment setup \n","\n","# GUIs don't work on the cloud, so we will supress wxPython: \n","%cd /content/cloned-DLC-repo\n","os.environ[\"DLClight\"]=\"True\"\n","os.environ[\"Colab\"]=\"True\"\n","\n","import deeplabcut\n","\n","# Options: ImgRework; LR4Step; ImgReworkDS; SetScales2; DoubleHourglassCLR; InterSup; QDoubleHourglass; tInterSup; FsIsTripleHourglass; LR1Ext; ScmapRework; TripleHourglass\n","\n","\n","\n","# Create a path variable that links to the config file:\n","from pathlib import Path\n","path_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/config_colab.yaml'\n","path_pose_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/dlc-models/iteration-4/CheetahApr2-trainset95shuffle1/train/pose_cfg_colab.yaml'\n","path_extension = str(Path(path_pose_config_file).parents[4] / 'extension-models' / Path(path_pose_config_file).parents[2].stem / Path(path_pose_config_file).parents[1].stem / Path(path_pose_config_file).parents[0].stem)\n","path_eval = str(Path(path_pose_config_file).parents[4] / 'evaluation-results' / Path(path_pose_config_file).parents[2].stem / Path(path_pose_config_file).parents[1].stem)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n","Project loaded in colab-mode. Apparently Colab has trouble loading statsmodels, so the smoothing & outlier frame extraction is disabled. Sorry!\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","DLC loaded in light mode; you cannot use the labeling GUI!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FRxbETQ14ODh","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"oJ0op7aiWEmS","colab_type":"text"},"source":["# Auxiliary Function Modifications"]},{"cell_type":"code","metadata":{"id":"ebWuhnr1WDkH","colab_type":"code","colab":{}},"source":["def GetModelFolder(trainFraction,shuffle,cfg):\n","    Task = cfg['Task']\n","    date = cfg['date']\n","    iterate = 'iteration-'+str(cfg['iteration'])\n","    return Path('extension-models/'+ iterate+'/'+Task + date + '-trainset' + str(int(trainFraction * 100)) + 'shuffle' + str(shuffle))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jMUkborZblA","colab_type":"code","colab":{}},"source":["def GetScorerName(cfg,shuffle,trainFraction,trainingsiterations='unknown'):\n","    ''' Extract the scorer/network name for a particular shuffle, training fraction, etc. '''\n","    Task = cfg['Task']\n","    date = cfg['date']\n","    if trainingsiterations=='unknown':\n","        snapshotindex=cfg['snapshotindex']\n","        if cfg['snapshotindex'] == 'all':\n","            print(\"Changing snapshotindext to the last one -- plotting, videomaking, etc. should not be performed for all indices. For more selectivity enter the ordinal number of the snapshot you want (ie. 4 for the fifth) in the config file.\")\n","            snapshotindex = -1\n","        else:\n","            snapshotindex=cfg['snapshotindex']\n","\n","        modelfolder=os.path.join(cfg[\"project_path\"],str(GetModelFolder(trainFraction,shuffle,cfg)),'train')\n","        Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(modelfolder) if \"index\" in fn])\n","        increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","        Snapshots = Snapshots[increasing_indices]\n","        #dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))\n","        #dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])\n","        SNP=Snapshots[snapshotindex]\n","        trainingsiterations = (SNP.split(os.sep)[-1]).split('-')[-1]\n","\n","    scorer = 'DLC_Extension' + \"_resnet\" + str(cfg['resnet']) + \"_\" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)\n","    return scorer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w-8MKxAHcSCr","colab_type":"code","colab":{}},"source":["def make_results_file(final_result,evaluationfolder):\n","    \"\"\"\n","    Makes result file in .h5 and csv format and saves under evaluation_results directory\n","    \"\"\"\n","    col_names = [\"Model\",'Training Parameters',\"Training iterations:\",\"%Training dataset\",\"Shuffle number\",\" Train error(px)\",\" Test error(px)\",\"1st p-cutoff used\",\"Train error with 1st p-cutoff\",\"Test error with 1st p-cutoff\",\"2nd p-cutoff used\",\"Train error with 2nd p-cutoff\",\"Test error with 2nd p-cutoff\",\"3rd p-cutoff used\",\"Train error with 3rd p-cutoff\",\"Test error with 3rd p-cutoff\"]\n","    df = pd.DataFrame(final_result, columns = col_names)\n","    df.to_hdf(os.path.join(str(evaluationfolder) + '/combined-results' + '.h5'),'df_with_missing',format='table',mode='w')\n","    df.to_csv(os.path.join(str(evaluationfolder) + '/combined-results' + '.csv'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xlp_vECfpaOS","colab_type":"code","colab":{}},"source":["def count_number_trainable_params():\n","    '''\n","    Counts the number of trainable variables.\n","    '''\n","    tot_nb_params = 0\n","    for trainable_variable in TF.trainable_variables():\n","        shape = trainable_variable.get_shape() # e.g [D,F] or [W,H,C]\n","        current_nb_params = get_nb_params_shape(shape)\n","        tot_nb_params = tot_nb_params + current_nb_params\n","    return tot_nb_params\n","\n","def get_nb_params_shape(shape):\n","    '''\n","    Computes the total number of params for a given shap.\n","    Works for any number of shapes etc [D,F] or [W,H,C] computes D*F and W*H*C.\n","    '''\n","    nb_params = 1\n","    for dim in shape:\n","        nb_params = nb_params*int(dim)\n","    return nb_params "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPPloJxprDdI","colab_type":"text"},"source":["# Model Definiton and PoseNet Class"]},{"cell_type":"markdown","metadata":{"id":"PF1vhmoBnmA9","colab_type":"text"},"source":["## Model Definition "]},{"cell_type":"markdown","metadata":{"id":"ib1jEnU7yoak","colab_type":"text"},"source":["### Bottleneck Module Definition\n","\n"]},{"cell_type":"code","metadata":{"id":"v2H2plJ2ynwq","colab_type":"code","colab":{}},"source":["from tensorflow.contrib import layers\n","from tensorflow.contrib.framework.python.ops import add_arg_scope\n","from tensorflow.contrib.layers.python.layers import utils\n","from tensorflow.contrib.slim.python.slim.nets import resnet_utils\n","from tensorflow.python.ops import nn_ops\n","from tensorflow.python.ops import variable_scope\n","\n","@add_arg_scope\n","def bottleneck(inputs,\n","               depth,\n","               depth_bottleneck,\n","               stride,\n","               rate=1,\n","               outputs_collections=None,\n","               scope=None):\n","  \"\"\"Bottleneck residual unit variant with BN after convolutions.\n","  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n","  its definition. Note that we use here the bottleneck variant which has an\n","  extra bottleneck layer.\n","  When putting together two consecutive ResNet blocks that use this unit, one\n","  should use stride = 2 in the last unit of the first block.\n","  Args:\n","    inputs: A tensor of size [batch, height, width, channels].\n","    depth: The depth of the ResNet unit output.\n","    depth_bottleneck: The depth of the bottleneck layers.\n","    stride: The ResNet unit's stride. Determines the amount of downsampling of\n","      the units output compared to its input.\n","    rate: An integer, rate for atrous convolution.\n","    outputs_collections: Collection to add the ResNet unit output.\n","    scope: Optional variable_scope.\n","  Returns:\n","    The ResNet unit's output.\n","  \"\"\"\n","  with variable_scope.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:\n","    depth_in = utils.last_dimension(inputs.get_shape(), min_rank=4)\n","    if depth == depth_in:   #PSE\n","      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n","    else:\n","        shortcut = layers.conv2d(\n","            inputs,\n","            depth, [1, 1],\n","            stride=stride,\n","            activation_fn=None,\n","            scope='shortcut')\n","    residual = layers.conv2d(\n","        inputs, depth_bottleneck, [1, 1], stride=1, scope='conv1')\n","    residual = resnet_utils.conv2d_same(\n","        residual, depth_bottleneck, 3, stride, rate=rate, scope='conv2')\n","    residual = layers.conv2d(\n","        residual, depth, [1, 1], stride=1, activation_fn=None, scope='conv3')\n","\n","    output = nn_ops.relu(shortcut + residual)\n","\n","    return utils.collect_named_outputs(outputs_collections, sc.name, output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbSomToBwj5W","colab_type":"text"},"source":["### Regnet Definition and Help functions"]},{"cell_type":"code","metadata":{"id":"h19_DM4yzyvc","colab_type":"code","colab":{}},"source":["# Helper functions for regnet\n","from tensorflow.contrib.layers.python.layers import layers as layers_lib\n","\n","def regnet_block_1(inputs,scope='regnet_block_1_'):\n","  net = layers.conv2d(\n","      inputs, 64, 7, stride=2,padding='SAME', scope=scope+'conv1')\n","  net = layers_lib.max_pool2d(\n","      net, [2, 2], stride=2, padding='SAME', scope=scope+'pool1')\n","  \n","  return net\n","\n","def regnet_block_2(inputs,\n","               depth_bottleneck,\n","               depth=256,\n","               stride=1,\n","               rate=1,\n","               scope=None):\n","  \n","  # Three bottleneck modules\n","  # Scope passed to function should include instance number\n","  if scope == None:\n","    net = bottleneck(\n","        inputs,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","  else:\n","    net = bottleneck(\n","        inputs,depth,depth_bottleneck,stride,rate,scope=scope+'_1')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_2')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_3')\n","\n","  return  net\n","\n","def regnet_block_3(inputs,\n","               depth_bottleneck,\n","               depth=256,\n","               stride=1,\n","               rate=1,\n","               scope=None):\n","  \n","  # Three bottleneck modules preceded by a maxpooling layer\n","  # Scope passed to function should include instance number\n","\n","  if scope == None:\n","    net = layers.max_pool2d(\n","        inputs, [2, 2], stride=2, padding='SAME')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)     \n","  else:\n","    net = layers.max_pool2d(\n","        inputs, [2, 2], stride=2, padding='SAME', scope=scope+'pool')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_1')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_2')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_3')    \n","\n","  return net\n","\n","def regnet_block_4(inputs,\n","               depth_bottleneck,\n","               scope=None):\n","  \n","  # Helper function to pass straight to regnet_block_2\n","  # Assists in scope handling\n","  net = regnet_block_2(\n","      inputs, depth_bottleneck=depth_bottleneck, scope=scope)\n","\n","  return net\n","\n","def regnet_block_5(inputs,\n","               depth,\n","               scope=None):\n","  \n","  net = layers.conv2d_transpose(\n","      inputs, depth, [2, 2], stride=2, padding='SAME', scope=scope)   \n","\n","  return net\n","\n","def regnet_block_6(inputs,\n","               depth,\n","               scope=None):\n","\n","  net = layers.conv2d(\n","        inputs, depth, [1, 1], stride=1, activation_fn=None, normalizer_fn=None, scope=scope, padding='SAME')   #toMod, activation and norm functions?\n","  \n","  return net\n","\n","def regnet_block_7(inputs,\n","               num_outputs,\n","               scope=None):\n","  \n","  net = layers.conv2d(                                                                              \n","        inputs, num_outputs, [1, 1], stride=1, activation_fn=None, normalizer_fn=None, scope=scope, padding='SAME')   #toMod, activation and norm functions?\n","\n","  return net\n","\n","def regnet_block_8(inputs,\n","               num_outputs,\n","               scope=None):\n","\n","  net = layers.conv2d_transpose(\n","    inputs, num_outputs, [2, 2], stride=2, scope=scope, padding='SAME')\n","  \n","  return net  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgykOG-RH5MU","colab_type":"code","colab":{}},"source":["# Hourglass construction helper function\n","def hourglass(inputs, hg_depth, scope='HG_'):\n","  assert hg_depth >= 1\n","  hg_level = '_'+str(hg_depth)\n","\n","  net = regnet_block_3(inputs, depth_bottleneck=128, scope=scope+'regnet_block_3'+hg_level)\n","\n","  if hg_depth == 1: \n","    shortcut = regnet_block_5(net, depth=256, scope=scope+'regnet_block_5'+hg_level)\n","  else:\n","    shortcut = regnet_block_5(hourglass(net,hg_depth-1,scope=scope), depth=256, scope=scope+'regnet_block_5'+hg_level)\n","\n","  residual = regnet_block_4(inputs, depth_bottleneck=128, scope=scope+'regnet_block_4'+hg_level)\n","\n","  return residual + shortcut     \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JqujkFWAm7dB","colab":{}},"source":["# Regnet function definition\n","\n","# Eval Parameters\n","# eval_interm = False\n","# eval_hourglass_stack = 1\n","# eval_full_scale = False\n","\n","# models = ['ImgRework', 'LR4Step', 'DoubleHourglassCLR', 'InterSup', 'QDoubleHourglass', 'tInterSup', 'LR1Ext', 'ScmapRework','DLC']\n","# models.append('TripleHourglass')\n","# models.append('SetScales2')\n","# models.append('ImgReworkDS')\n","# models.append('FsIsTripleHourglass')\n","\n","\n","def regnet(inputs, scmap_inputs, num_outputs, model_name):\n","  if model_name == 'SetScales2':\n","    inputs = regnet_block_1(inputs)  \n","    stacked_input = tf.concat([inputs,scmap_inputs],-1)\n","\n","    net = regnet_block_2(stacked_input, depth_bottleneck=64, scope='regnet_block_2_1')\n","    net = hourglass(net, hg_depth=4)\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_1')\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_2')\n","    net = regnet_block_7(net, num_outputs=num_outputs, scope='regnet_block_7_1')  \n","    net = regnet_block_8(net, num_outputs=num_outputs, scope='regnet_block_8_1')  \n","\n","    return net \n","  else: \n","    if model_name == 'DoubleHourglassCLR' or model_name == 'TripleHourglass' or model_name == 'FsIsTripleHourglass':\n","      eval_full_scale = True\n","    else: \n","      eval_full_scale = False\n","\n","    if model_name == 'InterSup' or model_name == 'tInterSup' or model_name == 'FsIsTripleHourglass':\n","      eval_interm = True\n","    else: \n","      eval_interm = False\n","\n","    if model_name == 'DoubleHourglassCLR' or model_name == 'InterSup' or model_name == 'QDoubleHourglass':\n","      eval_hourglass_stack = 2\n","    elif model_name == 'TripleHourglass' or model_name == 'tInterSup' or model_name == 'FsIsTripleHourglass':\n","      eval_hourglass_stack = 3\n","    else: \n","      eval_hourglass_stack = 1\n","\n","    #Input and Root Blocks\n","    inputs = regnet_block_1(inputs)  \n","    stacked_input = tf.concat([inputs,scmap_inputs],-1)\n","    net = regnet_block_2(stacked_input, depth_bottleneck=64, scope='regnet_block_2_1')\n","\n","    if eval_interm:\n","      hg_dict = {}\n","\n","    #Stacked Hourglass Definition\n","    for hg_num in range(1,eval_hourglass_stack+1):\n","      hg_name = str(hg_num)\n","      net = hourglass(net, hg_depth=4,scope='HG{}_'.format(hg_name))\n","\n","      #Intermediate Supervision Outputs\n","      if eval_interm and (hg_num != eval_hourglass_stack):\n","        hg_dict[hg_name] = regnet_block_6(net, depth=512, scope='hg{}_block_6_1'.format(hg_num))\n","        hg_dict[hg_name] = regnet_block_6(hg_dict[hg_name], depth=512, scope='hg{}_block_6_2'.format(hg_num))\n","        hg_dict[hg_name] = regnet_block_7(hg_dict[hg_name], num_outputs=num_outputs, scope='hg{}_block_7_1'.format(hg_num))  \n","        if eval_full_scale:\n","          hg_dict[hg_name] = regnet_block_8(hg_dict[hg_name], num_outputs=num_outputs, scope='hg{}_block_8_1'.format(hg_num))  \n","\n","    #Finishing Blocks\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_1')\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_2')\n","    net = regnet_block_7(net, num_outputs=num_outputs, scope='regnet_block_7_1')  \n","    if eval_full_scale:\n","      net = regnet_block_8(net, num_outputs=num_outputs, scope='regnet_block_8_1')  #Comment out for half scale, uncomment for full scale\n","\n","    # hg1_out = regnet_block_6(hg1_net, depth=512, scope='hg1_block_6_1')\n","    # hg1_out = regnet_block_6(hg1_out, depth=512, scope='hg1_block_6_2')\n","    # hg1_out = regnet_block_7(hg1_out, num_outputs=num_outputs, scope='hg1_block_7_1')  \n","    # # hg1_out = regnet_block_8(hg1_out, num_outputs=num_outputs, scope='hg1_block_8_1')  #Comment out for half scale, uncomment for full scale\n","\n","    # hg2_out = regnet_block_6(hg2_net, depth=512, scope='hg2_block_6_1')\n","    # hg2_out = regnet_block_6(hg2_out, depth=512, scope='hg2_block_6_2')\n","    # hg2_out = regnet_block_7(hg2_out, num_outputs=num_outputs, scope='hg2_block_7_1')  \n","    # # hg2_out = regnet_block_8(hg2_out, num_outputs=num_outputs, scope='hg2_block_8_1')  #Comment out for half scale, uncomment for full scale\n","\n","    if eval_interm:\n","      hg_outputs = []\n","      for i in range(eval_hourglass_stack-1):\n","        hg_outputs.append(hg_dict[str(i+1)])\n","      return net, hg_outputs\n","    else:\n","      return net\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RdU-YvAlUYH","colab_type":"text"},"source":["## PoseNet Class Definition"]},{"cell_type":"markdown","metadata":{"id":"6gB4Zx7CcvNx","colab_type":"text"},"source":["### prediction_layer"]},{"cell_type":"code","metadata":{"id":"0LVyEd7Tc0ov","colab_type":"code","colab":{}},"source":["# prediction_layer() function\n","#toMod -> should scope arg be changed here? -> happy with their layer hyper-parameters?\n","\n","import tensorflow.contrib.slim as slim\n","\n","def prediction_layer(cfg, input, name, num_outputs):\n","    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME',\n","                        activation_fn=None, normalizer_fn=None,\n","                        weights_regularizer=slim.l2_regularizer(cfg.weight_decay)):\n","        with TF.variable_scope(name):\n","            pred = layers.conv2d_transpose(input, num_outputs,            # changed slim to layers\n","                                         kernel_size=[2, 2], stride=2,    #adapted kernal stride\n","                                         scope='regnet_pred_layers', padding='SAME')    #inserted padding for sanity check\n","            return pred\n","\n","def dlc_prediction_layer(cfg, input, name, num_outputs):\n","    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME',\n","                        activation_fn=None, normalizer_fn=None,\n","                        weights_regularizer=slim.l2_regularizer(cfg.weight_decay)):\n","        with TF.variable_scope(name):\n","            pred = slim.conv2d_transpose(input, num_outputs,\n","                                         kernel_size=[3, 3], stride=2,\n","                                         scope='block4')\n","            return pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4X6AzLmos5_o","colab_type":"text"},"source":["### PoseNet"]},{"cell_type":"code","metadata":{"id":"tNWL38kMkg1f","colab_type":"code","colab":{}},"source":["# PoseNet Class\n","# Mod for interm sup (also to throw away hg_output for eval)\n","\n","from tensorflow.contrib.slim.python.slim.nets.resnet_utils import resnet_arg_scope\n","import tensorflow.contrib.slim as slim\n","from tensorflow.contrib.slim.nets import resnet_v1\n","\n","\n","class PoseNet:\n","    def __init__(self, dlc_cfg):\n","        self.dlc_cfg = dlc_cfg\n","\n","    def extract_features(self, inputs, scmap_inputs,model_name):\n","        if model_name == 'InterSup' or model_name == 'tInterSup' or model_name == 'FsIsTripleHourglass':\n","          eval_interm = True\n","        else: \n","          eval_interm = False\n","\n","        mean = tf.constant(self.dlc_cfg.mean_pixel,\n","                           dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean') \n","        im_centered = inputs - mean             #toMod, PSE, -> what is this doing? What should it be? Should I do the same with scmap_inputs\n","\n","        # The next part of the code depends upon which tensorflow version you have.\n","        vers = tf.__version__\n","        vers = vers.split(\".\") #Updated based on https://github.com/AlexEMG/DeepLabCut/issues/44\n","        if int(vers[0])==1 and int(vers[1])<4: #check if lower than version 1.4.\n","            with slim.arg_scope(resnet_arg_scope(False)):\n","              if model_name == 'DLC':\n","                net, end_points = resnet_v1.resnet_v1_50(im_centered, global_pool=False, output_stride=8)\n","                return net, end_points\n","              if eval_interm:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints, model_name)[0]\n","              else:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints, model_name)\n","        else:\n","            with slim.arg_scope(resnet_arg_scope()):\n","              if model_name == 'DLC':\n","                net, end_points = resnet_v1.resnet_v1_50(im_centered, global_pool=False, output_stride=8,is_training=False)\n","                return net, end_points\n","              if eval_interm:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints, model_name)[0]\n","              else:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints, model_name)\n","        return net\n","\n","    def prediction_layers(self, features, reuse=None):\n","        dlc_cfg = self.dlc_cfg\n","\n","        out = {}\n","        with TF.variable_scope('pose', reuse=reuse):\n","            out['part_pred'] = prediction_layer(dlc_cfg, features, 'part_pred',\n","                                                dlc_cfg.num_joints)\n","            if dlc_cfg.location_refinement:\n","                out['locref'] = prediction_layer(dlc_cfg, features, 'locref_pred',\n","                                                 dlc_cfg.num_joints * 2)\n","    \n","        return out\n","\n","    def dlc_prediction_layers(self, features, end_points, reuse=None):\n","        dlc_cfg = self.dlc_cfg\n","        num_layers = re.findall(\"resnet_([0-9]*)\", dlc_cfg.net_type)[0]\n","        layer_name = 'resnet_v1_{}'.format(num_layers) + '/block{}/unit_{}/bottleneck_v1'\n","\n","        out = {}\n","        with tf.variable_scope('pose', reuse=reuse):\n","            out['part_pred'] = dlc_prediction_layer(dlc_cfg, features, 'part_pred',\n","                                                dlc_cfg.num_joints)\n","            if dlc_cfg.location_refinement:\n","                out['locref'] = dlc_prediction_layer(dlc_cfg, features, 'locref_pred',\n","                                                 dlc_cfg.num_joints * 2)\n","            if dlc_cfg.intermediate_supervision:\n","                if dlc_cfg.net_type=='resnet_50' and dlc_cfg.intermediate_supervision_layer>6:\n","                    print(\"Changing layer to 6! (higher ones don't exist in block 3 of ResNet 50).\")\n","                    dlc_cfg.intermediate_supervision_layer=6\n","                interm_name = layer_name.format(3, dlc_cfg.intermediate_supervision_layer)\n","                block_interm_out = end_points[interm_name]\n","                out['part_pred_interm'] = dlc_prediction_layer(dlc_cfg, block_interm_out,\n","                                                           'intermediate_supervision',\n","                                                           dlc_cfg.num_joints)\n","        return out\n","\n","\n","\n","    def get_net(self, inputs, scmap_inputs, model_name):\n","        if model_name == 'DLC':\n","          net, end_points = self.extract_features(inputs, scmap_inputs, model_name)\n","          return self.dlc_prediction_layers(net, end_points)\n","        net = self.extract_features(inputs, scmap_inputs, model_name)\n","        return self.prediction_layers(net)\n","\n","    def test(self, inputs, scmap_inputs, model_name):\n","        heads = self.get_net(inputs, scmap_inputs, model_name)\n","        prob = tf.sigmoid(heads['part_pred'])\n","        return {'part_prob': prob, 'locref': heads['locref']}\n","\n","    def train(self, batch):\n","        dlc_cfg = self.dlc_cfg\n","\n","        if dlc_cfg.deterministic:\n","            tf.set_random_seed(42)\n","\n","        heads = self.get_net(batch[Batch.inputs], batch[Batch.scmap_inputs])\n","\n","        weigh_part_predictions = dlc_cfg.weigh_part_predictions\n","        part_score_weights = batch[Batch.part_score_weights] if weigh_part_predictions else 1.0\n","\n","        def add_part_loss(pred_layer):\n","            return TF.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets],\n","                                                   heads[pred_layer],\n","                                                   part_score_weights)\n","\n","        loss = {}\n","        loss['part_loss'] = add_part_loss('part_pred')\n","        total_loss = loss['part_loss']\n","\n","        if dlc_cfg.location_refinement:\n","            locref_pred = heads['locref']\n","            locref_targets = batch[Batch.locref_targets]\n","            locref_weights = batch[Batch.locref_mask]\n","\n","            loss_func = losses.huber_loss if dlc_cfg.locref_huber_loss else tf.losses.mean_squared_error\n","            loss['locref_loss'] = dlc_cfg.locref_loss_weight * loss_func(locref_targets, locref_pred, locref_weights)\n","            total_loss = total_loss + loss['locref_loss']\n","\n","    #     # loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize)\n","        loss['total_loss'] = total_loss\n","        return loss\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVlWBwfNur1A","colab_type":"text"},"source":["# Predicted Data Loading"]},{"cell_type":"code","metadata":{"id":"dq41wrfSSN7g","colab_type":"code","outputId":"e8c4a9a8-1ed7-4746-a1f1-d23ed46909a5","executionInfo":{"status":"ok","timestamp":1571035648306,"user_tz":-120,"elapsed":160754,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%cd /content/cloned-DLC-repo\n","\n","from math import floor, ceil    \n","import pandas as pd\n","from skimage import io\n","import skimage.color\n","import tensorflow as tf\n","from tqdm import tqdm\n","from scipy.misc import imresize\n","import time     #debug\n","import cv2\n","import numpy as np\n","import re\n","\n","from deeplabcut.utils import auxiliaryfunctions \n","from deeplabcut.pose_estimation_tensorflow.evaluate import pairwisedistances\n","from deeplabcut.pose_estimation_tensorflow.nnet import predict as ptf_predict\n","from deeplabcut.pose_estimation_tensorflow.config import load_config\n","from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import data_to_input\n","\n","predicted_data = {}\n","model_snapshots = {}\n","\n","###\n","# Parameters from function\n","###\n","\n","shuffle = 1\n","comparisonbodyparts=\"all\"\n","show_errors = True\n","gputouse=None\n","\n","\n","###\n","# Function\n","###\n","\n","if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:\n","    del os.environ['TF_CUDNN_USE_AUTOTUNE'] #was potentially set during training\n","\n","vers = (tf.__version__).split('.')\n","if int(vers[0])==1 and int(vers[1])>12:\n","    TF=tf.compat.v1\n","else:\n","    TF=tf\n","\n","TF.reset_default_graph()\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #\n","\n","start_path=os.getcwd()\n","# Read file path for pose_config file. >> pass it on\n","cfg = auxiliaryfunctions.read_config(path_config_file)\n","if gputouse is not None: #gpu selectinon\n","        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)\n","\n","# Loading human annotatated data\n","trainingsetfolder=auxiliaryfunctions.GetTrainingSetFolder(cfg)\n","Data=pd.read_hdf(os.path.join(cfg[\"project_path\"],str(trainingsetfolder),'CollectedData_' + cfg[\"scorer\"] + '.h5'),'df_with_missing')\n","# Get list of body parts to evaluate network for\n","comparisonbodyparts=auxiliaryfunctions.IntersectionofBodyPartsandOnesGivenbyUser(cfg,comparisonbodyparts)\n","# Make folder for evaluation\n","auxiliaryfunctions.attempttomakefolder(str(cfg[\"project_path\"]+\"/evaluation-results/\"))\n","\n","models = ['ImgRework', 'LR4Step', 'DoubleHourglassCLR', 'InterSup', 'QDoubleHourglass', 'tInterSup', 'LR1Ext', 'ScmapRework','DLC']\n","models.append('TripleHourglass')\n","models.append('SetScales2')\n","# models.append('ImgReworkDS')\n","models.append('FsIsTripleHourglass')\n","\n","final_result=[]\n","for model in models:\n","    for trainFraction in cfg[\"TrainingFraction\"]:\n","        ##################################################\n","        # Load and setup CNN part detector\n","        ##################################################\n","        datafn,metadatafn=auxiliaryfunctions.GetDataandMetaDataFilenames(trainingsetfolder,trainFraction,shuffle,cfg)\n","        # modelfolder=os.path.join(cfg[\"project_path\"],str(GetModelFolder(trainFraction,shuffle,cfg)))    #JO Function\n","        # path_train_config = Path(modelfolder) / 'train' / 'pose_cfg_colab.yaml'     #debug, should modify to test config\n","        # Load dataset meta data\n","        data, trainIndices, testIndices, trainFraction=auxiliaryfunctions.LoadMetadata(os.path.join(cfg[\"project_path\"],metadatafn))  #wrong, PSE, <- whas going on here with indices \n","\n","        try:\n","            dlc_cfg = load_config(str(path_pose_config_file))\n","        except FileNotFoundError:\n","            raise FileNotFoundError(\"It seems the model for shuffle %s and trainFraction %s does not exist.\"%(shuffle,trainFraction))\n","\n","        #change batch size, if it was edited during analysis!\n","        dlc_cfg['batch_size']=1 #in case this was edited for analysis.\n","        #Create folder structure to store results.\n","        evaluationfolder=os.path.join(cfg[\"project_path\"],str(auxiliaryfunctions.GetEvaluationFolder(trainFraction,shuffle,cfg)),model)\n","        auxiliaryfunctions.attempttomakefolder(evaluationfolder,recursive=True)\n","        #path_test_config = modelfolder / 'test' / 'pose_cfg_colab.yaml'\n","\n","        # Check which snapshots are available and sort them by # iterations\n","        model_location = path_extension + '/' + model\n","        if model == 'DLC':\n","          model_location = path_extension\n","\n","        Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(os.path.join(str(model_location), ''))if \"index\" in fn])\n","        try: #check if any were found?\n","          Snapshots[0]\n","        except IndexError:\n","          raise FileNotFoundError(\"Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\\nPlease train it before evaluating.\\nUse the function 'train_network' to do so.\"%(shuffle,trainFraction))\n","\n","        increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","        Snapshots = Snapshots[increasing_indices]\n","\n","        if cfg[\"snapshotindex\"] == -1:\n","            snapindex = [-1]\n","        else:\n","          raise ValueError\n","\n","        model_snapshots[model] = os.path.join(str(model_location),Snapshots[snapindex][0])\n","\n","        TF.reset_default_graph()\n","        inputs = TF.placeholder(tf.float32, shape=[dlc_cfg.batch_size   , None, None, 3])\n","        scoremap_and_locref_channel_multiplier = 3      #JO\n","        scmap_inputs = TF.placeholder(tf.float32, shape=[dlc_cfg.batch_size   , None, None, dlc_cfg.num_joints * scoremap_and_locref_channel_multiplier])   #JO\n","        net_heads = PoseNet(dlc_cfg).test(inputs, scmap_inputs, model)\n","        outputs = [net_heads['part_prob']]\n","        if dlc_cfg.location_refinement:\n","          outputs.append(net_heads['locref'])\n","\n","        restorer = TF.train.Saver()\n","        with tf.Session() as sess:\n","            sess.run(TF.global_variables_initializer())\n","            sess.run(TF.local_variables_initializer())\n","            restorer.restore(sess, model_snapshots[model])\n","            training_parameters = count_number_trainable_params()\n","\n","        TF.reset_default_graph()\n","\n","        trainingsiterations = (os.path.join(str(model_location),Snapshots[snapindex][0]).split(os.sep)[-1]).split('-')[-1] #read how many training siterations that corresponds to.\n","        DLCscorer = GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)     #JO Function\n","        if model == 'DLC':\n","          DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)[0]\n","\n","        resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + Snapshots[snapindex][0]+  '.h5')\n","        if model == 'DLC':\n","          resultsfilename = path_eval + '/DLC_resnet50_CheetahApr2shuffle1_1000000-snapshot-1000000.h5'\n","\n","        DataMachine = pd.read_hdf(resultsfilename,'df_with_missing')\n","\n","        predicted_data[model] = DataMachine\n","\n","        print(\"Loaded Predict data for snapshot: \", Snapshots[snapindex][0])\n","        DataCombined = pd.concat([Data.T, DataMachine.T], axis=0).T\n","        RMSE,RMSEpcutoff1 = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,0.5,comparisonbodyparts)\n","        RMSE,RMSEpcutoff2 = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,0.25,comparisonbodyparts)\n","        RMSE,RMSEpcutoff3 = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,0.1,comparisonbodyparts)\n","\n","        testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())\n","        trainerror = np.nanmean(RMSE.iloc[trainIndices].values.flatten())\n","\n","        testerrorpcutoff1 = np.nanmean(RMSEpcutoff1.iloc[testIndices].values.flatten())\n","        trainerrorpcutoff1 = np.nanmean(RMSEpcutoff1.iloc[trainIndices].values.flatten())\n","\n","        testerrorpcutoff2 = np.nanmean(RMSEpcutoff2.iloc[testIndices].values.flatten())\n","        trainerrorpcutoff2 = np.nanmean(RMSEpcutoff2.iloc[trainIndices].values.flatten())\n","\n","        testerrorpcutoff3 = np.nanmean(RMSEpcutoff3.iloc[testIndices].values.flatten())\n","        trainerrorpcutoff3 = np.nanmean(RMSEpcutoff3.iloc[trainIndices].values.flatten())\n","\n","        results = [model,training_parameters,trainingsiterations,int(100 * trainFraction),shuffle,np.round(trainerror,2),np.round(testerror,2),0.5,np.round(trainerrorpcutoff1,2), np.round(testerrorpcutoff1,2),0.25,np.round(trainerrorpcutoff2,2), np.round(testerrorpcutoff2,2),0.1,np.round(trainerrorpcutoff3,2), np.round(testerrorpcutoff3,2)]\n","        final_result.append(results)\n","\n","        if model == 'ImgRework':\n","          snapindex = [-41]\n","          model_snapshots[model+'_250'] = os.path.join(str(model_location),Snapshots[snapindex][0])\n","          trainingsiterations = (os.path.join(str(model_location),Snapshots[snapindex][0]).split(os.sep)[-1]).split('-')[-1] #read how many training siterations that corresponds to.\n","\n","          DLCscorer = GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)     #JO Function\n","\n","          resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + Snapshots[snapindex][0]+  '.h5')\n","\n","          DataMachine = pd.read_hdf(resultsfilename,'df_with_missing')\n","\n","          predicted_data[model+'_250'] = DataMachine\n","\n","          print(\"Loaded Predict data for snapshot: \", Snapshots[snapindex][0])\n","          DataCombined = pd.concat([Data.T, DataMachine.T], axis=0).T\n","          RMSE,RMSEpcutoff1 = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,0.5,comparisonbodyparts)\n","          RMSE,RMSEpcutoff2 = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,0.25,comparisonbodyparts)\n","          RMSE,RMSEpcutoff3 = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,0.1,comparisonbodyparts)\n","\n","          testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())\n","          trainerror = np.nanmean(RMSE.iloc[trainIndices].values.flatten())\n","\n","          testerrorpcutoff1 = np.nanmean(RMSEpcutoff1.iloc[testIndices].values.flatten())\n","          trainerrorpcutoff1 = np.nanmean(RMSEpcutoff1.iloc[trainIndices].values.flatten())\n","\n","          testerrorpcutoff2 = np.nanmean(RMSEpcutoff2.iloc[testIndices].values.flatten())\n","          trainerrorpcutoff2 = np.nanmean(RMSEpcutoff2.iloc[trainIndices].values.flatten())\n","\n","          testerrorpcutoff3 = np.nanmean(RMSEpcutoff3.iloc[testIndices].values.flatten())\n","          trainerrorpcutoff3 = np.nanmean(RMSEpcutoff3.iloc[trainIndices].values.flatten())\n","\n","          results = [model,training_parameters,trainingsiterations,int(100 * trainFraction),shuffle,np.round(trainerror,2),np.round(testerror,2),0.5,np.round(trainerrorpcutoff1,2), np.round(testerrorpcutoff1,2),0.25,np.round(trainerrorpcutoff2,2), np.round(testerrorpcutoff2,2),0.1,np.round(trainerrorpcutoff3,2), np.round(testerrorpcutoff3,2)]\n","          final_result.append(results)\n","\n","\n","        # if show_errors == True:\n","                # print(\"Results for\",trainingsiterations,\" training iterations:\", int(100 * trainFraction), shuffle, \"train error:\",np.round(trainerror,2), \"pixels. Test error:\", np.round(testerror,2),\" pixels.\")\n","                # print(\"With pcutoff of\", cfg[\"pcutoff\"],\" train error:\",np.round(trainerrorpcutoff,2), \"pixels. Test error:\", np.round(testerrorpcutoff,2), \"pixels\")\n","                # print(\"Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\\n\")\n","\n","make_results_file(final_result,path_eval)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/  already exists!\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/ImgRework  already exists!\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/ImgRework/IRWK_snapshot-500000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/ImgRework/IRWK_snapshot-500000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  IRWK_snapshot-500000\n","Loaded Predict data for snapshot:  IRWK_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/LR4Step  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/LR4Step/LR_4_step_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/LR4Step/LR_4_step_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  LR_4_step_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/DoubleHourglassCLR  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/DoubleHourglassCLR/DHG_CLR_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/DoubleHourglassCLR/DHG_CLR_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  DHG_CLR_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/InterSup  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/InterSup/ISDHG_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/InterSup/ISDHG_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  ISDHG_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/QDoubleHourglass  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/QDoubleHourglass/QDHG_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/QDoubleHourglass/QDHG_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  QDHG_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/tInterSup  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/tInterSup/IS_THG_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/tInterSup/IS_THG_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  IS_THG_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/LR1Ext  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/LR1Ext/LR_1ext_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/LR1Ext/LR_1ext_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  LR_1ext_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/ScmapRework  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/ScmapRework/SMR_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/ScmapRework/SMR_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  SMR_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/DLC  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/snapshot-1000000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/snapshot-1000000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  snapshot-1000000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/TripleHourglass  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/TripleHourglass/THG_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/TripleHourglass/THG_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  THG_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:156: RuntimeWarning: Mean of empty slice\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:157: RuntimeWarning: Mean of empty slice\n"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/SetScales2  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/SetScales2/ss2_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/SetScales2/ss2_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  ss2_snapshot-250000\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/FsIsTripleHourglass  already exists!\n","INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/FsIsTripleHourglass/FS_IS_THG_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/FsIsTripleHourglass/FS_IS_THG_snapshot-250000\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded Predict data for snapshot:  FS_IS_THG_snapshot-250000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IbxJJvYfcA2D","colab_type":"text"},"source":["# Image Plotting"]},{"cell_type":"code","metadata":{"id":"oM5HWOZlcEUa","colab_type":"code","colab":{}},"source":["from deeplabcut.utils import visualization\n","\n","colors = visualization.get_cmap(len(comparisonbodyparts),name=cfg['colormap'])\n","\n","print_models = []\n","# print_models.append('ImgRework')\n","\n","for model in print_models:\n","  print(\"Plotting for\",model)\n","\n","  model_location = path_extension + '/' + model\n","\n","  Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(os.path.join(str(model_location), ''))if \"index\" in fn])\n","  try: #check if any were found?\n","    Snapshots[0]\n","  except IndexError:\n","    raise FileNotFoundError(\"Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\\nPlease train it before evaluating.\\nUse the function 'train_network' to do so.\"%(shuffle,trainFraction))\n","\n","  increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","  Snapshots = Snapshots[increasing_indices]\n","  snapindex = [-1]\n","\n","  trainingsiterations = (os.path.join(str(model_location),Snapshots[snapindex][0]).split(os.sep)[-1]).split('-')[-1] #read how many training siterations that corresponds to.\n","  DLCscorer = GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)     #JO Function\n","\n","  evaluationfolder=os.path.join(cfg[\"project_path\"],str(auxiliaryfunctions.GetEvaluationFolder(trainFraction,shuffle,cfg)),model)\n","  foldername=os.path.join(str(evaluationfolder),'LabeledImages_' + DLCscorer + '_' + Snapshots[snapindex][0])\n","\n","  PrintDataMachine = predicted_data[model]\n","  DataCombined = pd.concat([Data.T, PrintDataMachine.T], axis=0).T\n","\n","  auxiliaryfunctions.attempttomakefolder(foldername)\n","  NumFrames=np.size(DataCombined.index)\n","  for ind in np.arange(NumFrames):\n","      visualization.PlottingandSaveLabeledFrame(DataCombined,ind,trainIndices,cfg,colors,comparisonbodyparts,DLCscorer,foldername)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pI5wZ_FoLDn2","colab_type":"text"},"source":["## Evaluate_Network Function"]},{"cell_type":"code","metadata":{"id":"hTMbaZgUuYbC","colab_type":"code","colab":{}},"source":["# # Adapted from DLC Evaluate_Network\n","# # deeplabcut.evaluate_network(path_config_file,plotting=True) #debug\n","# # def evaluate_network(config,Shuffles=[1],plotting = None,show_errors = True,comparisonbodyparts=\"all\",gputouse=None): #debug \n","\n","\n","# from math import floor, ceil    \n","# import pandas as pd\n","# from skimage import io\n","# import skimage.color\n","# import tensorflow as tf\n","# from tqdm import tqdm\n","# from scipy.misc import imresize\n","# import time     #debug\n","# import cv2\n","# import numpy as np\n","\n","\n","# from deeplabcut.utils import auxiliaryfunctions \n","# from deeplabcut.pose_estimation_tensorflow.evaluate import pairwisedistances, make_results_file\n","# from deeplabcut.pose_estimation_tensorflow.nnet import predict as ptf_predict\n","# from deeplabcut.pose_estimation_tensorflow.config import load_config\n","# from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import data_to_input\n","\n","# ###\n","# # Parameters from function\n","# ###\n","\n","# model_location = path_extension\n","# Shuffles=[1]\n","# comparisonbodyparts=\"all\"\n","# show_errors = True\n","# gputouse=None\n","# jo_interm_sup = False   \n","\n","# # global scmap  #hack\n","# # global locref\n","# # global pose\n","\n","# ###\n","# # Function\n","# ###\n","\n","# if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:\n","#     del os.environ['TF_CUDNN_USE_AUTOTUNE'] #was potentially set during training\n","\n","# vers = (tf.__version__).split('.')\n","# if int(vers[0])==1 and int(vers[1])>12:\n","#     TF=tf.compat.v1\n","# else:\n","#     TF=tf\n","\n","# TF.reset_default_graph()\n","\n","# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #\n","\n","# start_path=os.getcwd()\n","# # Read file path for pose_config file. >> pass it on\n","# cfg = auxiliaryfunctions.read_config(path_config_file)\n","# if gputouse is not None: #gpu selectinon\n","#         os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)\n","\n","# # Loading human annotatated data\n","# trainingsetfolder=auxiliaryfunctions.GetTrainingSetFolder(cfg)\n","# Data=pd.read_hdf(os.path.join(cfg[\"project_path\"],str(trainingsetfolder),'CollectedData_' + cfg[\"scorer\"] + '.h5'),'df_with_missing')\n","# # Get list of body parts to evaluate network for\n","# comparisonbodyparts=auxiliaryfunctions.IntersectionofBodyPartsandOnesGivenbyUser(cfg,comparisonbodyparts)\n","# # Make folder for evaluation\n","# auxiliaryfunctions.attempttomakefolder(str(cfg[\"project_path\"]+\"/evaluation-results/\"))\n","# for shuffle in Shuffles:\n","#     for trainFraction in cfg[\"TrainingFraction\"]:\n","#         ##################################################\n","#         # Load and setup CNN part detector\n","#         ##################################################\n","#         datafn,metadatafn=auxiliaryfunctions.GetDataandMetaDataFilenames(trainingsetfolder,trainFraction,shuffle,cfg)\n","#         modelfolder=os.path.join(cfg[\"project_path\"],str(GetModelFolder(trainFraction,shuffle,cfg)))    #JO Function\n","#         path_train_config = Path(modelfolder) / 'train' / 'pose_cfg_colab.yaml'     #debug, should modify to test config\n","#         # Load dataset meta data\n","#         data, trainIndices, testIndices, trainFraction=auxiliaryfunctions.LoadMetadata(os.path.join(cfg[\"project_path\"],metadatafn))  #wrong, PSE, <- whas going on here with indices \n","\n","#         try:\n","#             dlc_cfg = load_config(str(path_train_config))\n","#         except FileNotFoundError:\n","#             raise FileNotFoundError(\"It seems the model for shuffle %s and trainFraction %s does not exist.\"%(shuffle,trainFraction))\n","\n","#         #change batch size, if it was edited during analysis!\n","#         dlc_cfg['batch_size']=1 #in case this was edited for analysis.\n","#         #Create folder structure to store results.\n","#         evaluationfolder=os.path.join(cfg[\"project_path\"],str(auxiliaryfunctions.GetEvaluationFolder(trainFraction,shuffle,cfg)),model_version)\n","#         auxiliaryfunctions.attempttomakefolder(evaluationfolder,recursive=True)\n","#         #path_test_config = modelfolder / 'test' / 'pose_cfg_colab.yaml'\n","\n","#         # Check which snapshots are available and sort them by # iterations\n","#         Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(os.path.join(str(model_location), ''))if \"index\" in fn])\n","#         try: #check if any were found?\n","#           Snapshots[0]\n","#         except IndexError:\n","#           raise FileNotFoundError(\"Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\\nPlease train it before evaluating.\\nUse the function 'train_network' to do so.\"%(shuffle,trainFraction))\n","\n","#         increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","#         Snapshots = Snapshots[increasing_indices]\n","\n","#         if cfg[\"snapshotindex\"] == -1:\n","#             snapindices = [-1]\n","#         elif cfg[\"snapshotindex\"] == \"all\":\n","#             snapindices = range(len(Snapshots))\n","#         elif cfg[\"snapshotindex\"]<len(Snapshots):\n","#             snapindices=[cfg[\"snapshotindex\"]]\n","#         else:\n","#             print(\"Invalid choice, only -1 (last), any integer up to last, or all (as string)!\")\n","\n","#         final_result=[]\n","#         ##################################################\n","#         # Compute predictions over images\n","#         ##################################################\n","#         for snapindex in snapindices:\n","#             dlc_cfg['init_weights'] = os.path.join(str(model_location),Snapshots[snapindex]) #setting weights to corresponding snapshot.\n","#             trainingsiterations = (dlc_cfg['init_weights'].split(os.sep)[-1]).split('-')[-1] #read how many training siterations that corresponds to.\n","\n","#             #name for deeplabcut net (based on its parameters)\n","#             DLCscorer = GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)     #JO Function\n","#             print(\"Running \", DLCscorer, \" with # of trainingiterations:\", trainingsiterations)\n","#             resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + Snapshots[snapindex]+  '.h5')\n","#             try:\n","#                 DataMachine = pd.read_hdf(resultsfilename,'df_with_missing')\n","#                 print(\"This net has already been evaluated!\")\n","#             except FileNotFoundError:\n","#                 # Specifying state of model (snapshot / training state)\n","#                 sess, inputs, scmap_inputs, outputs = setup_pose_prediction(dlc_cfg)\n","\n","#                 #JO cobble analysis helper code\n","#                 Numimages = len(Data.index)\n","#                 start_index = 0\n","#                 # list_img_wo_scoremaps = []\n","#                 if 'PredicteData' not in locals():\n","#                   PredicteData = np.zeros((Numimages,3 * len(dlc_cfg['all_joints_names'])))\n","#                 # else:\n","#                 #   hold = 0\n","#                 #   hold_index = 0\n","#                 #   for j in range(Numimages+1):\n","#                 #     if np.sum(PredicteData[j]) == 0:\n","#                 #       if j-hold_index>1:\n","#                 #         hold = 0\n","#                 #       hold_index = j\n","#                 #       hold += 1\n","#                 #       if hold >=7:\n","#                 #         start_index = j - 8\n","#                 #         print('Some of PredicteData calculated. Starting at image index',start_index)\n","#                 #         break\n","#                 # if 'images_without_scmaps' not in locals():\n","#                 #   images_without_scmaps = 0\n","#                 #   print('images_without_scmaps will only count from this runs start index')\n","#                 # else:\n","#                 #   print('Some images_without_scmaps already found ({})'.format(images_without_scmaps))\n","\n","#                 print(\"Analyzing data...\")\n","#                 # got = False #hack\n","#                 for imageindex, imagename in tqdm(enumerate(Data.index)):\n","#                     # if got: #hack\n","#                     #   continue\n","#                     # imagename = 'labeled-data/27_02_2019EbonyFlick1CAM1/img007.png' #hack\n","\n","#                     if imageindex < start_index:  \n","#                       continue\n","\n","#                     save_name = str(dlc_cfg.project_path / Path(imagename).parents[0] / Path(imagename).stem)\n","#                     try:\n","#                       scmaps_loaded = np.load(save_name + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)  \n","#                     except FileNotFoundError:\n","#                       # print('Eval_Img_Loading')\n","#                       save_name = str(dlc_cfg.project_path / Path(imagename).parents[0] / ('eval_' + Path(imagename).stem))\n","#                       scmaps_loaded = np.load(save_name + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)\n","                      \n","#                       # list_img_wo_scoremaps.append(imagename)\n","#                       # images_without_scmaps += 1\n","#                       # print('\\nImage {} has no scmaps. Total now: {}'.format(Path(imagename).parents[0].stem + \"/\" + Path(imagename).stem,images_without_scmaps))\n","#                       # continue\n","                    \n","#                     # t0 = time.time()  #debug\n","#                     image = io.imread(os.path.join(cfg['project_path'],imagename),mode='RGB') \n","#                     image = skimage.color.gray2rgb(image)\n","#                     if image.shape[0]==1080:\n","#                       scale = 0.5334903964194936\n","#                     elif image.shape[0]==1520:\n","#                       # continue #debug\n","#                       scale = 0.7579494437963867/2 \n","#                     else:\n","#                       raise ValueError(\"Image Dimensions are not of a dimension which were expected (some hardcoding involved)\")                   \n","#                     img = imresize(image, scale)\n","#                     # if scale == 0.7579494437963867:\n","#                     #   img = img[:,1:,:]\n","#                     image_batch = data_to_input(img)\n","\n","#                     locref = scmaps_loaded['locref']\n","#                     scmap = scmaps_loaded['scmap']\n","#                     scmaps_concat = np.concatenate((locref[:,:,:,0],locref[:,:,:,1],scmap),axis=2)\n","#                     scmaps_concat = cv2.resize(scmaps_concat, dsize=(256, 144), interpolation=cv2.INTER_CUBIC)\n","#                     scmap_batch = data_to_input(scmaps_concat)\n","\n","#                     #PSE, #cobble, temp, TODO, wrong\n","#                     # dim0_increase_needed = ceil((floor((img.shape[0]-1)/2)+1)/2)-scmaps_concat.shape[0]\n","#                     # dim1_increase_needed = ceil((floor((img.shape[1]-1)/2)+1)/2)-scmaps_concat.shape[1]\n","#                     # # print('Debug: Img shape: {}\\tScoremap shape: {}\\tdim0: {}\\tdim1: {}\\n'.format(img.shape,scmaps_concat.shape,dim0_increase_needed,dim1_increase_needed))\n","#                     # scmap_padded = np.pad(scmaps_concat,[[floor(dim0_increase_needed/2),dim0_increase_needed-floor(dim0_increase_needed/2)],[floor(dim1_increase_needed/2),dim1_increase_needed-floor(dim1_increase_needed/2)],[0,0]],mode='constant')                    \n","#                     # scmap_batch = data_to_input(scmap_padded)\n","\n","#                     # t1 = time.time()\n","#                     # print('Data load and process took: ',t1-t0) #debug\n","\n","#                     # Compute prediction with the CNN\n","#                     outputs_np = sess.run(outputs, feed_dict={inputs: image_batch,scmap_inputs: scmap_batch})\n","#                     # t2 = time.time()\n","#                     # print('Session run took: ',t2-t1) #debug\n","\n","\n","#                     scmap, locref = ptf_predict.extract_cnn_output(outputs_np, dlc_cfg)\n","\n","#                     # Extract maximum scoring location from the heatmap, assume 1 person\n","#                     if eval_full_scale:\n","#                       scale_multiplier = 1\n","#                     else:\n","#                       scale_multiplier = 2\n","                    \n","#                     pose = ptf_predict.argmax_pose_predict(scmap, locref, scale_multiplier*(1/scale))\n","\n","#                     # got = True  #hack\n","#                     # continue\n","                    \n","#                     # t3 = time.time()\n","#                     # print('Pose Extraction took: ',t3-t2) #debug\n","#                     PredicteData[imageindex, :] = pose.flatten()  # NOTE: thereby     cfg_test['all_joints_names'] should be same order as bodyparts!\n","\n","#                     if imageindex % 500 == 0 and imageindex != 0:\n","#                       print('Saving data on index',imageindex)\n","#                       progress_save_name = evaluationfolder + '/{}_'.format(model_version)+str(trainingsiterations)+'_PredicteData-index'+str(imageindex)\n","#                       np.save(progress_save_name,PredicteData, allow_pickle=False, fix_imports=False)\n","#                 sess.close() #closes the current tf session\n","\n","#                 index = pd.MultiIndex.from_product(\n","#                     [[DLCscorer], dlc_cfg['all_joints_names'], ['x', 'y', 'likelihood']],\n","#                     names=['scorer', 'bodyparts', 'coords'])\n","\n","#                 # Saving results\n","#                 DataMachine = pd.DataFrame(PredicteData, columns=index, index=Data.index.values)\n","#                 DataMachine.to_hdf(resultsfilename,'df_with_missing',format='table',mode='w')\n","\n","#                 print(\"Done and results stored for snapshot: \", Snapshots[snapindex])\n","#                 DataCombined = pd.concat([Data.T, DataMachine.T], axis=0).T\n","#                 RMSE,RMSEpcutoff = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,cfg[\"pcutoff\"],comparisonbodyparts)\n","#                 testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())\n","#                 trainerror = np.nanmean(RMSE.iloc[trainIndices].values.flatten())\n","#                 testerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[testIndices].values.flatten())\n","#                 trainerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[trainIndices].values.flatten())\n","#                 results = [trainingsiterations,int(100 * trainFraction),shuffle,np.round(trainerror,2),np.round(testerror,2),cfg[\"pcutoff\"],np.round(trainerrorpcutoff,2), np.round(testerrorpcutoff,2)]\n","#                 final_result.append(results)\n","\n","#                 if show_errors == True:\n","#                         print(\"Results for\",trainingsiterations,\" training iterations:\", int(100 * trainFraction), shuffle, \"train error:\",np.round(trainerror,2), \"pixels. Test error:\", np.round(testerror,2),\" pixels.\")\n","#                         print(\"With pcutoff of\", cfg[\"pcutoff\"],\" train error:\",np.round(trainerrorpcutoff,2), \"pixels. Test error:\", np.round(testerrorpcutoff,2), \"pixels\")\n","#                         print(\"Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\")\n","\n","\n","#                 if plotting == True:\n","#                     print(\"Plotting...\")\n","#                     colors = visualization.get_cmap(len(comparisonbodyparts),name=cfg['colormap'])\n","\n","#                     foldername=os.path.join(str(evaluationfolder),'LabeledImages_' + DLCscorer + '_' + Snapshots[snapindex])\n","#                     auxiliaryfunctions.attempttomakefolder(foldername)\n","#                     NumFrames=np.size(DataCombined.index)\n","#                     for ind in np.arange(NumFrames):\n","#                         visualization.PlottingandSaveLabeledFrame(DataCombined,ind,trainIndices,cfg,colors,comparisonbodyparts,DLCscorer,foldername)\n","\n","#                 TF.reset_default_graph()\n","#                 #print(final_result)\n","#         make_results_file(final_result,evaluationfolder,DLCscorer)\n","#         print(\"The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\")\n","#         print(\"If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\\nUse the function 'analyze_video' to make predictions on new videos.\")\n","#         print(\"Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\")\n","\n","# # returning to intial folder\n","# os.chdir(str(start_path))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuIsu_ihi1w9","colab_type":"text"},"source":["# Text Log Loading"]},{"cell_type":"code","metadata":{"id":"A2fmNHqri4y7","colab_type":"code","colab":{}},"source":["training_logs = {}\n","\n","predicted_models = models.copy()\n","predicted_models.append('ImgRework_250')\n","\n","for model in predicted_models:\n","  training_logs[model] = np.genfromtxt(path_eval + '/TrainingLogs/{}.txt'.format(model),dtype=str)\n","  lines = len(training_logs[model])\n","  print('Loaded {} lines for {}'.format(lines,model))\n","\n","  if model == 'DLC':\n","    iteration_len = 1000\n","    iteration_offset = 0\n","    index_offset = 2\n","  elif model == 'ImgRework':\n","    iteration_len = 250\n","    iteration_offset = 250\n","    index_offset = 0\n","  else: \n","    iteration_len = 250\n","    iteration_offset = 0\n","    index_offset = 0\n","\n","  offset = 1\n","  for line in range(1,iteration_len+1):\n","    if training_logs[model][line-offset][1+index_offset] != str(1000*(line+iteration_offset)):\n","      print('No record for {} iteration {}'.format(model,1000*line))\n","      offset += 1\n","    if line == 250 and offset != 1:\n","      print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSeYl75JvsLU","colab_type":"text"},"source":["# Log Plotting"]},{"cell_type":"code","metadata":{"id":"Is0ZuifqvOOK","colab_type":"code","colab":{}},"source":["#Generate Plotting Data\n","#['ImgRework', 'LR4Step','DoubleHourglassCLR','InterSup','QDoubleHourglass','tInterSup','LR1Ext','ScmapRework','DLC','TripleHourglass','SetScales2','ImgRework_250',FsIsTripleHourglass']\n","\n","numpy_arrs = {}\n","\n","for model in predicted_models:\n","  entries = training_logs[model].shape[0]\n","  iters_arr = np.zeros(entries)\n","  loss_arr = np.zeros(entries)\n","  for line_idx in range(entries):\n","    line = training_logs[model][line_idx]\n","    if model == 'DLC':\n","      iters_arr[line_idx] = float(line[3])\n","      loss_arr[line_idx] = float(line[5])\n","      # iter_loss_list.append([int(line[3]),float(line[5])])\n","    else:\n","      iters_arr[line_idx] = float(line[1])\n","      loss_arr[line_idx] = float(line[3])\n","      # iter_loss_list.append([int(line[1]),float(line[3])])\n","  numpy_arrs[model+'_iters'] = iters_arr\n","  numpy_arrs[model+'_loss'] = loss_arr\n","\n","numpy_arrs['ImgRework_combined_iters'] = np.append(numpy_arrs['ImgRework_250'+'_iters'],numpy_arrs['ImgRework'+'_iters'])\n","numpy_arrs['ImgRework_combined_loss'] = np.append(numpy_arrs['ImgRework_250'+'_loss'],numpy_arrs['ImgRework'+'_loss'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nv_RBtoxxpvv","colab_type":"code","colab":{}},"source":["%cd '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/Plots/LossPlots'\n","\n","import matplotlib.pyplot as plt\n","#['ImgRework', 'LR4Step','DoubleHourglassCLR','InterSup','QDoubleHourglass','tInterSup','LR1Ext','ScmapRework','DLC','TripleHourglass','SetScales2','ImgRework_250']\n","\n","single_models_to_plot = predicted_models.copy()\n","single_models_to_plot = np.append(single_models_to_plot,['ImgRework_combined'])\n","multiple_models_to_plot = [['SetScales2','DoubleHourglassCLR','TripleHourglass'],['ImgRework_250','LR4Step','LR1Ext'],['ImgRework_250','InterSup','tInterSup']]\n","\n","for model in single_models_to_plot:\n","  print('Plotting',model)\n","  plt.figure(dpi=150, figsize=(10,5))\n","  # plt.semilogy(numpy_arrs['tInterSup'+'_iters'], numpy_arrs['tInterSup'+'_loss'],  color='r')\n","  plt.plot(numpy_arrs[model+'_iters'], numpy_arrs[model+'_loss'],  color='r')\n","  # plt.title(\"Training Error (Pixelwise Cross Enthalpy)\")\n","  plt.xlabel(\"Training Iterations\")\n","  plt.ylabel(\"Pixelwise Cross Enthalpy Error\")\n","  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n","\n","  plt.tight_layout()\n","\n","  plt.savefig(\"{}.png\".format(model))\n","  plt.close()\n","\n","counter = 0\n","colours = ['r','g','b']\n","for group in multiple_models_to_plot:\n","  print('Plotting',group)\n","  plt.figure(dpi=150, figsize=(10,5))\n","  idx = 0\n","  for model in group:\n","    if counter > 0:\n","      plt.semilogy(numpy_arrs[model+'_iters'], numpy_arrs[model+'_loss'],  color=colours[idx], label=model)\n","    else:\n","      plt.plot(numpy_arrs[model+'_iters'], numpy_arrs[model+'_loss'],  color=colours[idx], label=model)\n","    idx += 1\n","  # plt.title(\"Training Error (Pixelwise Cross Enthalpy)\")\n","  plt.xlabel(\"Training Iterations\")\n","  plt.ylabel(\"Pixelwise Cross Enthalpy Error\")\n","  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n","\n","  plt.tight_layout()\n","\n","  savename = 'joint_'\n","  first = True\n","  for model in group:\n","    if first == True:\n","      first = False\n","      savename = savename + model\n","    else:\n","      savename = savename + '&' + model\n","\n","  plt.savefig(\"{}.png\".format(savename))\n","  plt.close()\n","  counter += 1\n","\n","%cd /content/cloned-DLC-repo\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DySqz8wT9CC5","colab_type":"code","colab":{}},"source":["single_models_to_plot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgiGHZeFd3DV","colab_type":"code","colab":{}},"source":["try:\n","  files.download(\"../test.png\")\n","except ValueError:\n","  files.download(\"../test.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BkfLnfLUeP8J","colab_type":"code","colab":{}},"source":["# plt.plot(np.append(numpy_arrs['ImgRework_250'+'_iters'],numpy_arrs['ImgRework'+'_iters']), np.append(numpy_arrs['ImgRework_250'+'_loss'],numpy_arrs['ImgRework'+'_loss']),color='r')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Z7HVKTWQPeW","colab_type":"text"},"source":["# Example Scoremaps"]},{"cell_type":"code","metadata":{"id":"KaHDnTOG-XrL","colab_type":"code","colab":{}},"source":["%cd '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/iteration-4/CheetahApr2-trainset95shuffle1/Plots/ScoreMapExamples'\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","from matplotlib.colors import LogNorm\n","import copy\n","\n","for idx in range(25):\n","  print('Scoremap:',idx)\n","  meta_data = np.load('/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/labeled-data/09_03_2019JulesFlick1CAM3/img144.npz')\n","  scmap_example = meta_data['scmap'][:,:,idx]\n","  x = np.arange(scmap_example.shape[1])\n","  y = np.arange(scmap_example.shape[0])\n","\n","  my_cmap = copy.copy(cm.get_cmap('jet')) # copy the default cmap\n","  my_cmap.set_bad((0,0,0.25))  #rgb\n","                  \n","  plt.figure(dpi=150, figsize=(10,5))\n","  plt.imshow(scmap_example, extent=(np.amin(x), np.amax(x), np.amin(y), np.amax(y)),\n","          cmap=my_cmap, norm=LogNorm())\n","\n","  # X,Y = np.meshgrid(x,y)\n","  # plt.pcolormesh(X,Y,scmap_example, cmap=cm.jet, norm=LogNorm())\n","\n","  cbar = plt.colorbar()\n","  cbar.set_label('Probability of Presence of Bodypart', rotation=270, labelpad=20)\n","  plt.show()\n","\n","  plt.savefig(\"09_03_2019JulesFlick1CAM3_img144_sm{}.png\".format(idx))\n","  plt.close()\n","\n","print('Done ScoreMap generation')\n","\n","%cd /content/cloned-DLC-repo"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHTKV90pRkw5","colab_type":"code","colab":{}},"source":["  # idx = 0\n","  # print('Scoremap:',idx)\n","  # meta_data = np.load('/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/labeled-data/09_03_2019JulesFlick1CAM3/img144.npz')\n","  # scmap_example = meta_data['scmap'][:,:,idx]\n","  # x = np.arange(scmap_example.shape[1])\n","  # y = np.arange(scmap_example.shape[0])\n","\n","  # my_cmap = copy.copy(cm.get_cmap('jet')) # copy the default cmap\n","  # my_cmap.set_bad((0,0,0.25))  #rgb\n","                  \n","  # plt.figure(dpi=150, figsize=(10,5))\n","  # plt.imshow(scmap_example, extent=(np.amin(x), np.amax(x), np.amin(y), np.amax(y)),\n","  #         cmap=my_cmap, norm=LogNorm())\n","\n","  # # X,Y = np.meshgrid(x,y)\n","  # # plt.pcolormesh(X,Y,scmap_example, cmap=cm.jet, norm=LogNorm())\n","\n","  # cbar = plt.colorbar()\n","  # cbar.set_label('Probability of Presence of Bodypart', rotation=270, labelpad=20)\n","\n","  # plt.savefig(\"../09_03_2019JulesFlick1CAM3_img144_sm{}.png\".format(idx))\n","  # plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMkCYkooVQQd","colab_type":"code","colab":{}},"source":["# files.download('/content/09_03_2019JulesFlick1CAM3_img144_sm0.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVGjKKBdeBb_","colab_type":"code","colab":{}},"source":["print((Snapshots[-1]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBAPZu0OoCA-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}