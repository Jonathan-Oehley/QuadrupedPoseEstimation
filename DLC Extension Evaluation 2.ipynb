{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DLC Extension Evaluation 2.ipynb","provenance":[{"file_id":"1ERgsp6pTDFpZD6yLHAW-WQB8l1z50Kmx","timestamp":1570824175207}],"collapsed_sections":["StEH0RAw0mEm","snP56FXP0mEZ","PF1vhmoBnmA9","oJ0op7aiWEmS","rydUn5vHqlfZ","XscfUqOsq3v5","pI5wZ_FoLDn2"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"StEH0RAw0mEm"},"source":["# Setup Code"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k-vf1MaC0mEi"},"source":["### Drive Setup"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"91b0d47e-35b7-4bb9-fae6-753e00bc91cb","executionInfo":{"status":"ok","timestamp":1571004362399,"user_tz":-120,"elapsed":1324,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"id":"nUiuRcjp0mEb","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"snP56FXP0mEZ"},"source":["### DLC Setup\n","Code created to automatically crash the notebook to reload the dependencies that were imported. Requires manual comment of os.kill line of code after the first run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h4UoOUfI0mET","colab":{}},"source":["# Download and installation\n","%cd /content\n","!git clone -l -s git://github.com/AlexEMG/DeepLabCut.git cloned-DLC-repo\n","%cd cloned-DLC-repo\n","\n","from IPython.display import clear_output\n","# !pip install deeplabcut\n","clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n7Ae0ouo0mEN"},"source":["#### Setup.py write"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"5fdeebe1-a2e1-443a-95e9-683cbb3db6c3","executionInfo":{"status":"ok","timestamp":1571004365040,"user_tz":-120,"elapsed":3945,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"id":"_zsLk0fY0mEG","colab":{"base_uri":"https://localhost:8080/"}},"source":["%%writefile setup.py\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","DeepLabCut2.0 Toolbox (deeplabcut.org)\n","Â© A. & M. Mathis Labs\n","https://github.com/AlexEMG/DeepLabCut\n","\n","Please see AUTHORS for contributors.\n","https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS\n","Licensed under GNU Lesser General Public License v3.0\n","\"\"\"\n","\n","import setuptools\n","\n","with open(\"README.md\", \"r\") as fh:\n","    long_description = fh.read()\n","\n","setuptools.setup(\n","    name=\"deeplabcut\",\n","    version=\"2.0.9\",\n","    author=\"A. & M. Mathis Labs\",\n","    author_email=\"alexander.mathis@bethgelab.org\",\n","    description=\"Markerless pose-estimation of user-defined features with deep learning\",\n","    long_description=long_description,\n","    long_description_content_type=\"text/markdown\",\n","    url=\"https://github.com/AlexEMG/DeepLabCut\",\n","    install_requires=['certifi','chardet~=3.0.4','click','easydict~=1.7',\n","                      'gast==0.2.2','h5py~=2.7','imageio~=2.3.0','intel-openmp',\n","                      'ipython~=6.0.0','ipython-genutils~=0.2.0',\n","                      'matplotlib~=3.0.3','moviepy~=0.2.3.5','numpy~=1.14.5','opencv-python~=3.4',\n","                      'pandas>=0.21.0','patsy','python-dateutil~=2.7.3','pyyaml>=5.1','requests',\n","                      'ruamel.yaml~=0.15','setuptools','scikit-image~=0.14.0','scikit-learn~=0.19.2',\n","                      'scipy~=1.1.0','statsmodels~=0.9.0','tables',\n","                      'tensorpack~=0.9.7.1',\n","                      'tqdm>4.29','wheel~=0.31.1'],\n","    scripts=['deeplabcut/pose_estimation_tensorflow/models/pretrained/download.sh'],\n","    packages=setuptools.find_packages(),\n","    data_files=[('deeplabcut',['deeplabcut/pose_cfg.yaml','deeplabcut/pose_estimation_tensorflow/models/pretrained/pretrained_model_urls.yaml'])],\n","    include_package_data=True,\n","    classifiers=(\n","        \"Programming Language :: Python :: 3\",\n","        \"License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)\",\n","        \"Operating System :: OS Independent\",\n","    ),\n","    entry_points=\"\"\"[console_scripts]\n","            dlc=dlc:main\"\"\",\n",")\n","\n","#https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting setup.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TsaI-bRJ0mEE"},"source":["### Remaining Setup"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ODkhs7wc0mD4","colab":{}},"source":["%cd /content\n","!pip install -e cloned-DLC-repo\n","clear_output()\n","\n","import os\n","# os.kill(os.getpid(), 9)     # Comment this line out after first run\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HLmZcrZk0mDl","outputId":"1b5d12b4-3c3e-4844-8a3b-88f27302324c","executionInfo":{"status":"ok","timestamp":1571004374715,"user_tz":-120,"elapsed":13591,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Environment setup \n","\n","# GUIs don't work on the cloud, so we will supress wxPython: \n","%cd /content/cloned-DLC-repo\n","os.environ[\"DLClight\"]=\"True\"\n","os.environ[\"Colab\"]=\"True\"\n","\n","import deeplabcut\n","\n","# Create a path variable that links to the config file:\n","from pathlib import Path\n","path_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/config_colab.yaml'\n","path_pose_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/dlc-models/iteration-4/CheetahApr2-trainset95shuffle1/train/pose_cfg_colab.yaml'\n","# Options: Batch8; ImgRework; LR4Step; SetScales1; DoubleHourglass; ImgReworkDS; SetScales2; DoubleHourglassCLR; InterSup; QDoubleHourglass; tInterSup; FsIsTripleHourglass; LR1Ext; ScmapRework; TripleHourglass\n","model_version =  'FsIsTripleHourglass'\n","eval_interm = True\n","eval_hourglass_stack = 3\n","eval_full_scale = True\n","eval_snap_index = -1\n","path_extension = str(Path(path_pose_config_file).parents[4] / 'extension-models' / Path(path_pose_config_file).parents[2].stem / Path(path_pose_config_file).parents[1].stem / Path(path_pose_config_file).parents[0].stem / model_version)\n","plotting = False "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n","Project loaded in colab-mode. Apparently Colab has trouble loading statsmodels, so the smoothing & outlier frame extraction is disabled. Sorry!\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","DLC loaded in light mode; you cannot use the labeling GUI!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TVlWBwfNur1A","colab_type":"text"},"source":["# Evaluation"]},{"cell_type":"markdown","metadata":{"id":"PF1vhmoBnmA9","colab_type":"text"},"source":["## Model Definition "]},{"cell_type":"markdown","metadata":{"id":"ib1jEnU7yoak","colab_type":"text"},"source":["### Bottleneck Module Definition\n","\n"]},{"cell_type":"code","metadata":{"id":"v2H2plJ2ynwq","colab_type":"code","colab":{}},"source":["from tensorflow.contrib import layers\n","from tensorflow.contrib.framework.python.ops import add_arg_scope\n","from tensorflow.contrib.layers.python.layers import utils\n","from tensorflow.contrib.slim.python.slim.nets import resnet_utils\n","from tensorflow.python.ops import nn_ops\n","from tensorflow.python.ops import variable_scope\n","\n","@add_arg_scope\n","def bottleneck(inputs,\n","               depth,\n","               depth_bottleneck,\n","               stride,\n","               rate=1,\n","               outputs_collections=None,\n","               scope=None):\n","  \"\"\"Bottleneck residual unit variant with BN after convolutions.\n","  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n","  its definition. Note that we use here the bottleneck variant which has an\n","  extra bottleneck layer.\n","  When putting together two consecutive ResNet blocks that use this unit, one\n","  should use stride = 2 in the last unit of the first block.\n","  Args:\n","    inputs: A tensor of size [batch, height, width, channels].\n","    depth: The depth of the ResNet unit output.\n","    depth_bottleneck: The depth of the bottleneck layers.\n","    stride: The ResNet unit's stride. Determines the amount of downsampling of\n","      the units output compared to its input.\n","    rate: An integer, rate for atrous convolution.\n","    outputs_collections: Collection to add the ResNet unit output.\n","    scope: Optional variable_scope.\n","  Returns:\n","    The ResNet unit's output.\n","  \"\"\"\n","  with variable_scope.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:\n","    depth_in = utils.last_dimension(inputs.get_shape(), min_rank=4)\n","    if depth == depth_in:   #PSE\n","      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n","    else:\n","        shortcut = layers.conv2d(\n","            inputs,\n","            depth, [1, 1],\n","            stride=stride,\n","            activation_fn=None,\n","            scope='shortcut')\n","    residual = layers.conv2d(\n","        inputs, depth_bottleneck, [1, 1], stride=1, scope='conv1')\n","    residual = resnet_utils.conv2d_same(\n","        residual, depth_bottleneck, 3, stride, rate=rate, scope='conv2')\n","    residual = layers.conv2d(\n","        residual, depth, [1, 1], stride=1, activation_fn=None, scope='conv3')\n","\n","    output = nn_ops.relu(shortcut + residual)\n","\n","    return utils.collect_named_outputs(outputs_collections, sc.name, output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbSomToBwj5W","colab_type":"text"},"source":["### Regnet Definition and Help functions"]},{"cell_type":"code","metadata":{"id":"h19_DM4yzyvc","colab_type":"code","colab":{}},"source":["# Helper functions for regnet\n","from tensorflow.contrib.layers.python.layers import layers as layers_lib\n","\n","def regnet_block_1(inputs,scope='regnet_block_1_'):\n","  net = layers.conv2d(\n","      inputs, 64, 7, stride=2,padding='SAME', scope=scope+'conv1')\n","  net = layers_lib.max_pool2d(\n","      net, [2, 2], stride=2, padding='SAME', scope=scope+'pool1')\n","  \n","  return net\n","\n","def regnet_block_2(inputs,\n","               depth_bottleneck,\n","               depth=256,\n","               stride=1,\n","               rate=1,\n","               scope=None):\n","  \n","  # Three bottleneck modules\n","  # Scope passed to function should include instance number\n","  if scope == None:\n","    net = bottleneck(\n","        inputs,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","  else:\n","    net = bottleneck(\n","        inputs,depth,depth_bottleneck,stride,rate,scope=scope+'_1')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_2')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_3')\n","\n","  return  net\n","\n","def regnet_block_3(inputs,\n","               depth_bottleneck,\n","               depth=256,\n","               stride=1,\n","               rate=1,\n","               scope=None):\n","  \n","  # Three bottleneck modules preceded by a maxpooling layer\n","  # Scope passed to function should include instance number\n","\n","  if scope == None:\n","    net = layers.max_pool2d(\n","        inputs, [2, 2], stride=2, padding='SAME')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)     \n","  else:\n","    net = layers.max_pool2d(\n","        inputs, [2, 2], stride=2, padding='SAME', scope=scope+'pool')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_1')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_2')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_3')    \n","\n","  return net\n","\n","def regnet_block_4(inputs,\n","               depth_bottleneck,\n","               scope=None):\n","  \n","  # Helper function to pass straight to regnet_block_2\n","  # Assists in scope handling\n","  net = regnet_block_2(\n","      inputs, depth_bottleneck=depth_bottleneck, scope=scope)\n","\n","  return net\n","\n","def regnet_block_5(inputs,\n","               depth,\n","               scope=None):\n","  \n","  net = layers.conv2d_transpose(\n","      inputs, depth, [2, 2], stride=2, padding='SAME', scope=scope)   \n","\n","  return net\n","\n","def regnet_block_6(inputs,\n","               depth,\n","               scope=None):\n","\n","  net = layers.conv2d(\n","        inputs, depth, [1, 1], stride=1, activation_fn=None, normalizer_fn=None, scope=scope, padding='SAME')   #toMod, activation and norm functions?\n","  \n","  return net\n","\n","def regnet_block_7(inputs,\n","               num_outputs,\n","               scope=None):\n","  \n","  net = layers.conv2d(                                                                              \n","        inputs, num_outputs, [1, 1], stride=1, activation_fn=None, normalizer_fn=None, scope=scope, padding='SAME')   #toMod, activation and norm functions?\n","\n","  return net\n","\n","def regnet_block_8(inputs,\n","               num_outputs,\n","               scope=None):\n","\n","  net = layers.conv2d_transpose(\n","    inputs, num_outputs, [2, 2], stride=2, scope=scope, padding='SAME')\n","  \n","  return net  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgykOG-RH5MU","colab_type":"code","colab":{}},"source":["# Hourglass construction helper function\n","def hourglass(inputs, hg_depth, scope='HG_'):\n","  assert hg_depth >= 1\n","  hg_level = '_'+str(hg_depth)\n","\n","  net = regnet_block_3(inputs, depth_bottleneck=128, scope=scope+'regnet_block_3'+hg_level)\n","\n","  if hg_depth == 1: \n","    shortcut = regnet_block_5(net, depth=256, scope=scope+'regnet_block_5'+hg_level)\n","  else:\n","    shortcut = regnet_block_5(hourglass(net,hg_depth-1,scope=scope), depth=256, scope=scope+'regnet_block_5'+hg_level)\n","\n","  residual = regnet_block_4(inputs, depth_bottleneck=128, scope=scope+'regnet_block_4'+hg_level)\n","\n","  return residual + shortcut     \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JqujkFWAm7dB","colab":{}},"source":["# Regnet function definition\n","\n","# Eval Parameters\n","# eval_interm = False\n","# eval_hourglass_stack = 1\n","# eval_full_scale = False\n","\n","def regnet(inputs, scmap_inputs, num_outputs):\n","  if model_version == 'SetScales2':\n","    inputs = regnet_block_1(inputs)  \n","    stacked_input = tf.concat([inputs,scmap_inputs],-1)\n","\n","    net = regnet_block_2(stacked_input, depth_bottleneck=64, scope='regnet_block_2_1')\n","    net = hourglass(net, hg_depth=4)\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_1')\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_2')\n","    net = regnet_block_7(net, num_outputs=num_outputs, scope='regnet_block_7_1')  \n","    net = regnet_block_8(net, num_outputs=num_outputs, scope='regnet_block_8_1')  \n","\n","    return net \n","  else: \n","    #Input and Root Blocks\n","    inputs = regnet_block_1(inputs)  \n","    stacked_input = tf.concat([inputs,scmap_inputs],-1)\n","    net = regnet_block_2(stacked_input, depth_bottleneck=64, scope='regnet_block_2_1')\n","\n","    if eval_interm:\n","      hg_dict = {}\n","\n","    #Stacked Hourglass Definition\n","    for hg_num in range(1,eval_hourglass_stack+1):\n","      hg_name = str(hg_num)\n","      net = hourglass(net, hg_depth=4,scope='HG{}_'.format(hg_name))\n","\n","      #Intermediate Supervision Outputs\n","      if eval_interm and (hg_num != eval_hourglass_stack):\n","        hg_dict[hg_name] = regnet_block_6(net, depth=512, scope='hg{}_block_6_1'.format(hg_num))\n","        hg_dict[hg_name] = regnet_block_6(hg_dict[hg_name], depth=512, scope='hg{}_block_6_2'.format(hg_num))\n","        hg_dict[hg_name] = regnet_block_7(hg_dict[hg_name], num_outputs=num_outputs, scope='hg{}_block_7_1'.format(hg_num))  \n","        if eval_full_scale:\n","          hg_dict[hg_name] = regnet_block_8(hg_dict[hg_name], num_outputs=num_outputs, scope='hg{}_block_8_1'.format(hg_num))  \n","\n","    #Finishing Blocks\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_1')\n","    net = regnet_block_6(net, depth=512, scope='regnet_block_6_2')\n","    net = regnet_block_7(net, num_outputs=num_outputs, scope='regnet_block_7_1')  \n","    if eval_full_scale:\n","      net = regnet_block_8(net, num_outputs=num_outputs, scope='regnet_block_8_1')  #Comment out for half scale, uncomment for full scale\n","\n","    # hg1_out = regnet_block_6(hg1_net, depth=512, scope='hg1_block_6_1')\n","    # hg1_out = regnet_block_6(hg1_out, depth=512, scope='hg1_block_6_2')\n","    # hg1_out = regnet_block_7(hg1_out, num_outputs=num_outputs, scope='hg1_block_7_1')  \n","    # # hg1_out = regnet_block_8(hg1_out, num_outputs=num_outputs, scope='hg1_block_8_1')  #Comment out for half scale, uncomment for full scale\n","\n","    # hg2_out = regnet_block_6(hg2_net, depth=512, scope='hg2_block_6_1')\n","    # hg2_out = regnet_block_6(hg2_out, depth=512, scope='hg2_block_6_2')\n","    # hg2_out = regnet_block_7(hg2_out, num_outputs=num_outputs, scope='hg2_block_7_1')  \n","    # # hg2_out = regnet_block_8(hg2_out, num_outputs=num_outputs, scope='hg2_block_8_1')  #Comment out for half scale, uncomment for full scale\n","\n","    if eval_interm:\n","      hg_outputs = []\n","      for i in range(eval_hourglass_stack-1):\n","        hg_outputs.append(hg_dict[str(i+1)])\n","      return net, hg_outputs\n","    else:\n","      return net\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJ0op7aiWEmS","colab_type":"text"},"source":["## Auxiliary Function Modifications"]},{"cell_type":"code","metadata":{"id":"ebWuhnr1WDkH","colab_type":"code","colab":{}},"source":["def GetModelFolder(trainFraction,shuffle,cfg):\n","    Task = cfg['Task']\n","    date = cfg['date']\n","    iterate = 'iteration-'+str(cfg['iteration'])\n","    return Path('extension-models/'+ iterate+'/'+Task + date + '-trainset' + str(int(trainFraction * 100)) + 'shuffle' + str(shuffle))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jMUkborZblA","colab_type":"code","colab":{}},"source":["def GetScorerName(cfg,shuffle,trainFraction,trainingsiterations='unknown'):\n","    ''' Extract the scorer/network name for a particular shuffle, training fraction, etc. '''\n","    Task = cfg['Task']\n","    date = cfg['date']\n","    if trainingsiterations=='unknown':\n","        snapshotindex=cfg['snapshotindex']\n","        if cfg['snapshotindex'] == 'all':\n","            print(\"Changing snapshotindext to the last one -- plotting, videomaking, etc. should not be performed for all indices. For more selectivity enter the ordinal number of the snapshot you want (ie. 4 for the fifth) in the config file.\")\n","            snapshotindex = -1\n","        else:\n","            snapshotindex=cfg['snapshotindex']\n","\n","        modelfolder=os.path.join(cfg[\"project_path\"],str(GetModelFolder(trainFraction,shuffle,cfg)),'train')\n","        Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(modelfolder) if \"index\" in fn])\n","        increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","        Snapshots = Snapshots[increasing_indices]\n","        #dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))\n","        #dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])\n","        SNP=Snapshots[snapshotindex]\n","        trainingsiterations = (SNP.split(os.sep)[-1]).split('-')[-1]\n","\n","    scorer = 'DLC_Extension' + \"_resnet\" + str(cfg['resnet']) + \"_\" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)\n","    return scorer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rydUn5vHqlfZ","colab_type":"text"},"source":["## PoseNet Class "]},{"cell_type":"markdown","metadata":{"id":"gp6YUfEVO9a7","colab_type":"text"},"source":["### pose_net function definitions\n","Functions of the same name adapted from DeeperCut pose_net.py"]},{"cell_type":"markdown","metadata":{"id":"EKH0WX4alQL0","colab_type":"text"},"source":["#### get_batch_spec"]},{"cell_type":"code","metadata":{"id":"mIKnZZD3P4Nm","colab_type":"code","colab":{}},"source":["# get_batch_spec() function\n","\n","def get_batch_spec(cfg):\n","    num_joints = cfg.num_joints\n","    batch_size = cfg.batch_size\n","    scoremap_and_locref_channel_multiplier = 3\n","    return {  \n","        Batch.inputs: [batch_size, None, None, 3],\n","        Batch.part_score_targets: [batch_size, None, None, num_joints],\n","        Batch.part_score_weights: [batch_size, None, None, num_joints],\n","        Batch.locref_targets: [batch_size, None, None, num_joints * 2],\n","        Batch.locref_mask: [batch_size, None, None, num_joints * 2],\n","        Batch.scmap_inputs: [batch_size, None, None, num_joints * scoremap_and_locref_channel_multiplier]\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6gB4Zx7CcvNx","colab_type":"text"},"source":["#### prediction_layer"]},{"cell_type":"code","metadata":{"id":"0LVyEd7Tc0ov","colab_type":"code","colab":{}},"source":["# prediction_layer() function\n","#toMod -> should scope arg be changed here? -> happy with their layer hyper-parameters?\n","\n","import tensorflow.contrib.slim as slim\n","\n","def prediction_layer(cfg, input, name, num_outputs):\n","    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME',\n","                        activation_fn=None, normalizer_fn=None,\n","                        weights_regularizer=slim.l2_regularizer(cfg.weight_decay)):\n","        with TF.variable_scope(name):\n","            pred = layers.conv2d_transpose(input, num_outputs,            # changed slim to layers\n","                                         kernel_size=[2, 2], stride=2,    #adapted kernal stride\n","                                         scope='regnet_pred_layers', padding='SAME')    #inserted padding for sanity check\n","            return pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RdU-YvAlUYH","colab_type":"text"},"source":["#### PoseNet Class Definition"]},{"cell_type":"code","metadata":{"id":"tNWL38kMkg1f","colab_type":"code","colab":{}},"source":["# PoseNet Class\n","# Mod for interm sup (also to throw away hg_output for eval)\n","\n","from tensorflow.contrib.slim.python.slim.nets.resnet_utils import resnet_arg_scope\n","\n","class PoseNet:\n","    def __init__(self, dlc_cfg):\n","        self.dlc_cfg = dlc_cfg\n","\n","    def extract_features(self, inputs, scmap_inputs):\n","        mean = tf.constant(self.dlc_cfg.mean_pixel,\n","                           dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean') \n","        im_centered = inputs - mean             #toMod, PSE, -> what is this doing? What should it be? Should I do the same with scmap_inputs\n","\n","        # The next part of the code depends upon which tensorflow version you have.\n","        vers = tf.__version__\n","        vers = vers.split(\".\") #Updated based on https://github.com/AlexEMG/DeepLabCut/issues/44\n","        if int(vers[0])==1 and int(vers[1])<4: #check if lower than version 1.4.\n","            with slim.arg_scope(resnet_arg_scope(False)):\n","              if eval_interm:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints)[0]\n","              else:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints)\n","        else:\n","            with slim.arg_scope(resnet_arg_scope()):\n","              if eval_interm:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints)[0]\n","              else:\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints)\n","        return net\n","\n","    def prediction_layers(self, features, reuse=None):\n","        dlc_cfg = self.dlc_cfg\n","\n","        out = {}\n","        with TF.variable_scope('pose', reuse=reuse):\n","            out['part_pred'] = prediction_layer(dlc_cfg, features, 'part_pred',\n","                                                dlc_cfg.num_joints)\n","            if dlc_cfg.location_refinement:\n","                out['locref'] = prediction_layer(dlc_cfg, features, 'locref_pred',\n","                                                 dlc_cfg.num_joints * 2)\n","    \n","        return out\n","\n","    def get_net(self, inputs, scmap_inputs):\n","        net = self.extract_features(inputs, scmap_inputs)\n","        return self.prediction_layers(net)\n","\n","    def test(self, inputs, scmap_inputs):\n","        heads = self.get_net(inputs, scmap_inputs)\n","        prob = tf.sigmoid(heads['part_pred'])\n","        return {'part_prob': prob, 'locref': heads['locref']}\n","\n","    def train(self, batch):\n","        dlc_cfg = self.dlc_cfg\n","\n","        if dlc_cfg.deterministic:\n","            tf.set_random_seed(42)\n","\n","        heads = self.get_net(batch[Batch.inputs], batch[Batch.scmap_inputs])\n","\n","        weigh_part_predictions = dlc_cfg.weigh_part_predictions\n","        part_score_weights = batch[Batch.part_score_weights] if weigh_part_predictions else 1.0\n","\n","        def add_part_loss(pred_layer):\n","            return TF.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets],\n","                                                   heads[pred_layer],\n","                                                   part_score_weights)\n","\n","        loss = {}\n","        loss['part_loss'] = add_part_loss('part_pred')\n","        total_loss = loss['part_loss']\n","\n","        if dlc_cfg.location_refinement:\n","            locref_pred = heads['locref']\n","            locref_targets = batch[Batch.locref_targets]\n","            locref_weights = batch[Batch.locref_mask]\n","\n","            loss_func = losses.huber_loss if dlc_cfg.locref_huber_loss else tf.losses.mean_squared_error\n","            loss['locref_loss'] = dlc_cfg.locref_loss_weight * loss_func(locref_targets, locref_pred, locref_weights)\n","            total_loss = total_loss + loss['locref_loss']\n","\n","    #     # loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize)\n","        loss['total_loss'] = total_loss\n","        return loss\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XscfUqOsq3v5","colab_type":"text"},"source":["## setup_pose_predict Function"]},{"cell_type":"code","metadata":{"id":"Ze0qFemZq9R1","colab_type":"code","colab":{}},"source":["def setup_pose_prediction(dlc_cfg):\n","    TF.reset_default_graph()\n","    inputs = TF.placeholder(tf.float32, shape=[dlc_cfg.batch_size   , None, None, 3])\n","    scoremap_and_locref_channel_multiplier = 3      #JO\n","    scmap_inputs = TF.placeholder(tf.float32, shape=[dlc_cfg.batch_size   , None, None, dlc_cfg.num_joints * scoremap_and_locref_channel_multiplier])   #JO\n","    net_heads = PoseNet(dlc_cfg).test(inputs, scmap_inputs)\n","    outputs = [net_heads['part_prob']]\n","    if dlc_cfg.location_refinement:\n","        outputs.append(net_heads['locref'])\n","\n","    restorer = TF.train.Saver()\n","    sess = TF.Session()\n","    sess.run(TF.global_variables_initializer())\n","    sess.run(TF.local_variables_initializer())\n","\n","    # Restore variables from disk.\n","    restorer.restore(sess, dlc_cfg.init_weights)\n","\n","    return sess, inputs, scmap_inputs, outputs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pI5wZ_FoLDn2","colab_type":"text"},"source":["## Evaluate_Network Function"]},{"cell_type":"code","metadata":{"id":"hTMbaZgUuYbC","colab_type":"code","outputId":"574d4690-8484-42d7-e916-c0064b8beff1","executionInfo":{"status":"ok","timestamp":1571008794137,"user_tz":-120,"elapsed":451982,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02185926462316740964"}},"colab":{"base_uri":"https://localhost:8080/","height":564}},"source":["# Adapted from DLC Evaluate_Network\n","# deeplabcut.evaluate_network(path_config_file,plotting=True) #debug\n","# def evaluate_network(config,Shuffles=[1],plotting = None,show_errors = True,comparisonbodyparts=\"all\",gputouse=None): #debug \n","\n","%cd /content/cloned-DLC-repo\n","\n","import numpy as np\n","from math import floor, ceil    \n","import pandas as pd\n","from skimage import io\n","import skimage.color\n","import tensorflow as tf\n","from tqdm import tqdm\n","from scipy.misc import imresize\n","import time     #debug\n","import cv2\n","\n","\n","from deeplabcut.utils import auxiliaryfunctions \n","from deeplabcut.pose_estimation_tensorflow.evaluate import pairwisedistances, make_results_file\n","from deeplabcut.pose_estimation_tensorflow.nnet import predict as ptf_predict\n","from deeplabcut.pose_estimation_tensorflow.config import load_config\n","from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import data_to_input\n","\n","###\n","# Parameters from function\n","###\n","\n","model_location = path_extension\n","Shuffles=[1]\n","comparisonbodyparts=\"all\"\n","show_errors = True\n","gputouse=None\n","jo_interm_sup = False   \n","\n","###\n","# Function\n","###\n","\n","if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:\n","    del os.environ['TF_CUDNN_USE_AUTOTUNE'] #was potentially set during training\n","\n","vers = (tf.__version__).split('.')\n","if int(vers[0])==1 and int(vers[1])>12:\n","    TF=tf.compat.v1\n","else:\n","    TF=tf\n","\n","TF.reset_default_graph()\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #\n","\n","start_path=os.getcwd()\n","# Read file path for pose_config file. >> pass it on\n","cfg = auxiliaryfunctions.read_config(path_config_file)\n","if gputouse is not None: #gpu selectinon\n","        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)\n","\n","# Loading human annotatated data\n","trainingsetfolder=auxiliaryfunctions.GetTrainingSetFolder(cfg)\n","Data=pd.read_hdf(os.path.join(cfg[\"project_path\"],str(trainingsetfolder),'CollectedData_' + cfg[\"scorer\"] + '.h5'),'df_with_missing')\n","# Get list of body parts to evaluate network for\n","comparisonbodyparts=auxiliaryfunctions.IntersectionofBodyPartsandOnesGivenbyUser(cfg,comparisonbodyparts)\n","# Make folder for evaluation\n","auxiliaryfunctions.attempttomakefolder(str(cfg[\"project_path\"]+\"/evaluation-results/\"))\n","for shuffle in Shuffles:\n","    for trainFraction in cfg[\"TrainingFraction\"]:\n","        ##################################################\n","        # Load and setup CNN part detector\n","        ##################################################\n","        datafn,metadatafn=auxiliaryfunctions.GetDataandMetaDataFilenames(trainingsetfolder,trainFraction,shuffle,cfg)\n","        modelfolder=os.path.join(cfg[\"project_path\"],str(GetModelFolder(trainFraction,shuffle,cfg)))    #JO Function\n","        path_train_config = Path(modelfolder) / 'train' / 'pose_cfg_colab.yaml'     #debug, should modify to test config\n","        # Load dataset meta data\n","        data, trainIndices, testIndices, trainFraction=auxiliaryfunctions.LoadMetadata(os.path.join(cfg[\"project_path\"],metadatafn))  #wrong, PSE, <- whas going on here with indices \n","\n","        try:\n","            dlc_cfg = load_config(str(path_train_config))\n","        except FileNotFoundError:\n","            raise FileNotFoundError(\"It seems the model for shuffle %s and trainFraction %s does not exist.\"%(shuffle,trainFraction))\n","\n","        #change batch size, if it was edited during analysis!\n","        dlc_cfg['batch_size']=1 #in case this was edited for analysis.\n","        #Create folder structure to store results.\n","        evaluationfolder=os.path.join(cfg[\"project_path\"],str(auxiliaryfunctions.GetEvaluationFolder(trainFraction,shuffle,cfg)),model_version)\n","        auxiliaryfunctions.attempttomakefolder(evaluationfolder,recursive=True)\n","        #path_test_config = modelfolder / 'test' / 'pose_cfg_colab.yaml'\n","\n","        # Check which snapshots are available and sort them by # iterations\n","        Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(os.path.join(str(model_location), ''))if \"index\" in fn])\n","        try: #check if any were found?\n","          Snapshots[0]\n","        except IndexError:\n","          raise FileNotFoundError(\"Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\\nPlease train it before evaluating.\\nUse the function 'train_network' to do so.\"%(shuffle,trainFraction))\n","\n","        increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","        Snapshots = Snapshots[increasing_indices]\n","\n","        if cfg[\"snapshotindex\"] == -1:\n","            snapindices = [eval_snap_index]\n","        elif cfg[\"snapshotindex\"] == \"all\":\n","            snapindices = range(len(Snapshots))\n","        elif cfg[\"snapshotindex\"]<len(Snapshots):\n","            snapindices=[cfg[\"snapshotindex\"]]\n","        else:\n","            print(\"Invalid choice, only -1 (last), any integer up to last, or all (as string)!\")\n","\n","        final_result=[]\n","        ##################################################\n","        # Compute predictions over images\n","        ##################################################\n","        for snapindex in snapindices:\n","            dlc_cfg['init_weights'] = os.path.join(str(model_location),Snapshots[snapindex]) #setting weights to corresponding snapshot.\n","            trainingsiterations = (dlc_cfg['init_weights'].split(os.sep)[-1]).split('-')[-1] #read how many training siterations that corresponds to.\n","\n","            #name for deeplabcut net (based on its parameters)\n","            DLCscorer = GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)     #JO Function\n","            print(\"Running \", DLCscorer, \" with # of trainingiterations:\", trainingsiterations)\n","            resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + Snapshots[snapindex]+  '.h5')\n","            try:\n","                DataMachine = pd.read_hdf(resultsfilename,'df_with_missing')\n","                print(\"This net has already been evaluated!\")\n","            except FileNotFoundError:\n","                # Specifying state of model (snapshot / training state)\n","                sess, inputs, scmap_inputs, outputs = setup_pose_prediction(dlc_cfg)\n","\n","                #JO cobble analysis helper code\n","                Numimages = len(Data.index)\n","                start_index = 0\n","                # list_img_wo_scoremaps = []\n","                if 'PredicteData' not in locals():\n","                  PredicteData = np.zeros((Numimages,3 * len(dlc_cfg['all_joints_names'])))\n","                # else:\n","                #   hold = 0\n","                #   hold_index = 0\n","                #   for j in range(Numimages+1):\n","                #     if np.sum(PredicteData[j]) == 0:\n","                #       if j-hold_index>1:\n","                #         hold = 0\n","                #       hold_index = j\n","                #       hold += 1\n","                #       if hold >=7:\n","                #         start_index = j - 8\n","                #         print('Some of PredicteData calculated. Starting at image index',start_index)\n","                #         break\n","                # if 'images_without_scmaps' not in locals():\n","                #   images_without_scmaps = 0\n","                #   print('images_without_scmaps will only count from this runs start index')\n","                # else:\n","                #   print('Some images_without_scmaps already found ({})'.format(images_without_scmaps))\n","\n","                print(\"Analyzing data...\")\n","                for imageindex, imagename in tqdm(enumerate(Data.index)):\n","                    # continue #debug\n","                    if imageindex < start_index:  \n","                      continue\n","\n","                    save_name = str(dlc_cfg.project_path / Path(imagename).parents[0] / Path(imagename).stem)\n","                    try:\n","                      scmaps_loaded = np.load(save_name + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)  \n","                    except FileNotFoundError:\n","                      # print('Eval_Img_Loading')\n","                      save_name = str(dlc_cfg.project_path / Path(imagename).parents[0] / ('eval_' + Path(imagename).stem))\n","                      scmaps_loaded = np.load(save_name + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)\n","                      \n","                      # list_img_wo_scoremaps.append(imagename)\n","                      # images_without_scmaps += 1\n","                      # print('\\nImage {} has no scmaps. Total now: {}'.format(Path(imagename).parents[0].stem + \"/\" + Path(imagename).stem,images_without_scmaps))\n","                      # continue\n","                    \n","                    # t0 = time.time()  #debug\n","                    image = io.imread(os.path.join(cfg['project_path'],imagename),mode='RGB')\n","                    image = skimage.color.gray2rgb(image)\n","                    if image.shape[0]==1080:\n","                      scale = 0.5334903964194936\n","                    elif image.shape[0]==1520:\n","                      # continue #debug\n","                      scale = 0.7579494437963867/2 \n","                    else:\n","                      raise ValueError(\"Image Dimensions are not of a dimension which were expected (some hardcoding involved)\")                   \n","                    img = imresize(image, scale)\n","                    # if scale == 0.7579494437963867:\n","                    #   img = img[:,1:,:]\n","                    image_batch = data_to_input(img)\n","\n","                    locref = scmaps_loaded['locref']\n","                    scmap = scmaps_loaded['scmap']\n","                    scmaps_concat = np.concatenate((locref[:,:,:,0],locref[:,:,:,1],scmap),axis=2)\n","                    scmaps_concat = cv2.resize(scmaps_concat, dsize=(256, 144), interpolation=cv2.INTER_CUBIC)\n","                    scmap_batch = data_to_input(scmaps_concat)\n","\n","                    #PSE, #cobble, temp, TODO, wrong\n","                    # dim0_increase_needed = ceil((floor((img.shape[0]-1)/2)+1)/2)-scmaps_concat.shape[0]\n","                    # dim1_increase_needed = ceil((floor((img.shape[1]-1)/2)+1)/2)-scmaps_concat.shape[1]\n","                    # # print('Debug: Img shape: {}\\tScoremap shape: {}\\tdim0: {}\\tdim1: {}\\n'.format(img.shape,scmaps_concat.shape,dim0_increase_needed,dim1_increase_needed))\n","                    # scmap_padded = np.pad(scmaps_concat,[[floor(dim0_increase_needed/2),dim0_increase_needed-floor(dim0_increase_needed/2)],[floor(dim1_increase_needed/2),dim1_increase_needed-floor(dim1_increase_needed/2)],[0,0]],mode='constant')                    \n","                    # scmap_batch = data_to_input(scmap_padded)\n","\n","                    # t1 = time.time()\n","                    # print('Data load and process took: ',t1-t0) #debug\n","\n","                    # Compute prediction with the CNN\n","                    outputs_np = sess.run(outputs, feed_dict={inputs: image_batch,scmap_inputs: scmap_batch})\n","                    # t2 = time.time()\n","                    # print('Session run took: ',t2-t1) #debug\n","                    scmap, locref = ptf_predict.extract_cnn_output(outputs_np, dlc_cfg)\n","\n","                    # Extract maximum scoring location from the heatmap, assume 1 person\n","                    if eval_full_scale:\n","                      scale_multiplier = 1\n","                    else:\n","                      scale_multiplier = 2\n","\n","                    pose = ptf_predict.argmax_pose_predict(scmap, locref, scale_multiplier*(1/scale))\n","                    # t3 = time.time()\n","                    # print('Pose Extraction took: ',t3-t2) #debug\n","                    PredicteData[imageindex, :] = pose.flatten()  # NOTE: thereby     cfg_test['all_joints_names'] should be same order as bodyparts!\n","\n","                    if imageindex % 500 == 0 and imageindex != 0:\n","                      print('Saving data on index',imageindex)\n","                      progress_save_name = evaluationfolder + '/{}_'.format(model_version)+str(trainingsiterations)+'_PredicteData-index'+str(imageindex)\n","                      np.save(progress_save_name,PredicteData, allow_pickle=False, fix_imports=False)\n","                sess.close() #closes the current tf session\n","\n","                index = pd.MultiIndex.from_product( \n","                    [[DLCscorer], dlc_cfg['all_joints_names'], ['x', 'y', 'likelihood']],\n","                    names=['scorer', 'bodyparts', 'coords'])\n","\n","                # Saving results\n","                DataMachine = pd.DataFrame(PredicteData, columns=index, index=Data.index.values)\n","                DataMachine.to_hdf(resultsfilename,'df_with_missing',format='table',mode='w')\n","\n","                print(\"Done and results stored for snapshot: \", Snapshots[snapindex])\n","                DataCombined = pd.concat([Data.T, DataMachine.T], axis=0).T\n","                RMSE,RMSEpcutoff = pairwisedistances(DataCombined, cfg[\"scorer\"], DLCscorer,cfg[\"pcutoff\"],comparisonbodyparts)\n","                testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())\n","                trainerror = np.nanmean(RMSE.iloc[trainIndices].values.flatten())\n","                testerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[testIndices].values.flatten())\n","                trainerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[trainIndices].values.flatten())\n","                results = [trainingsiterations,int(100 * trainFraction),shuffle,np.round(trainerror,2),np.round(testerror,2),cfg[\"pcutoff\"],np.round(trainerrorpcutoff,2), np.round(testerrorpcutoff,2)]\n","                final_result.append(results)\n","\n","                if show_errors == True:\n","                        print(\"Results for\",trainingsiterations,\" training iterations:\", int(100 * trainFraction), shuffle, \"train error:\",np.round(trainerror,2), \"pixels. Test error:\", np.round(testerror,2),\" pixels.\")\n","                        print(\"With pcutoff of\", cfg[\"pcutoff\"],\" train error:\",np.round(trainerrorpcutoff,2), \"pixels. Test error:\", np.round(testerrorpcutoff,2), \"pixels\")\n","                        print(\"Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\")\n","\n","\n","                if plotting == True:\n","                    print(\"Plotting...\")\n","                    colors = visualization.get_cmap(len(comparisonbodyparts),name=cfg['colormap'])\n","\n","                    foldername=os.path.join(str(evaluationfolder),'LabeledImages_' + DLCscorer + '_' + Snapshots[snapindex])\n","                    auxiliaryfunctions.attempttomakefolder(foldername)\n","                    NumFrames=np.size(DataCombined.index)\n","                    for ind in np.arange(NumFrames):\n","                        visualization.PlottingandSaveLabeledFrame(DataCombined,ind,trainIndices,cfg,colors,comparisonbodyparts,DLCscorer,foldername)\n","\n","                TF.reset_default_graph()\n","                #print(final_result)\n","        make_results_file(final_result,evaluationfolder,DLCscorer)\n","        print(\"The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\")\n","        print(\"If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\\nUse the function 'analyze_video' to make predictions on new videos.\")\n","        print(\"Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\")\n","\n","# returning to intial folder\n","os.chdir(str(start_path))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n","/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/evaluation-results/  already exists!\n","Running  DLC_Extension_resnet50_CheetahApr2shuffle1_250000  with # of trainingiterations: 250000\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/FsIsTripleHourglass/FS_IS_THG_snapshot-250000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/extension-models/iteration-4/CheetahApr2-trainset95shuffle1/train/FsIsTripleHourglass/FS_IS_THG_snapshot-250000\n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Analyzing data...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n","501it [19:55,  1.52s/it]"],"name":"stderr"},{"output_type":"stream","text":["Saving data on index 500\n"],"name":"stdout"},{"output_type":"stream","text":["1001it [35:42,  1.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Saving data on index 1000\n"],"name":"stdout"},{"output_type":"stream","text":["1501it [47:44,  1.30s/it]"],"name":"stderr"},{"output_type":"stream","text":["Saving data on index 1500\n"],"name":"stdout"},{"output_type":"stream","text":["2001it [1:01:26,  4.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["Saving data on index 2000\n"],"name":"stdout"},{"output_type":"stream","text":["2501it [1:12:27,  1.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Saving data on index 2500\n"],"name":"stdout"},{"output_type":"stream","text":["2507it [1:12:31,  1.74s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Done and results stored for snapshot:  FS_IS_THG_snapshot-250000\n","Results for 250000  training iterations: 95 1 train error: 147.06 pixels. Test error: 153.39  pixels.\n","With pcutoff of 0.5  train error: nan pixels. Test error: nan pixels\n","Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n","The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n","If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n","Use the function 'analyze_video' to make predictions on new videos.\n","Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:235: RuntimeWarning: Mean of empty slice\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:236: RuntimeWarning: Mean of empty slice\n"],"name":"stderr"}]}]}