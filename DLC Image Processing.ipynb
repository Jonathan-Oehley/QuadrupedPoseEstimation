{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DLC Image Processing.ipynb","provenance":[{"file_id":"14JIWDvTIx5lhZfVEPULAe66FAcmNSZXL","timestamp":1569591365589}],"collapsed_sections":["w9zKBC2WMk47"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"w9zKBC2WMk47","colab_type":"text"},"source":["# Setup Code"]},{"cell_type":"markdown","metadata":{"id":"-1wCJICbsZ3p","colab_type":"text"},"source":["### Drive Setup"]},{"cell_type":"code","metadata":{"id":"_vCN9I7Gto0q","colab_type":"code","outputId":"b3971295-5436-4fc1-f45f-aa32139f4fea","executionInfo":{"status":"ok","timestamp":1570562762974,"user_tz":-120,"elapsed":1676,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02239620675032514891"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0FgtyiwnMtjs","colab_type":"text"},"source":["### DLC Setup\n","Code created to automatically crash the notebook to reload the dependencies that were imported. Requires manual comment of os.kill line of code after the first run"]},{"cell_type":"code","metadata":{"id":"wMp8pFAGMvSR","colab_type":"code","colab":{}},"source":["# Download and installation\n","%cd /content\n","!git clone -l -s git://github.com/AlexEMG/DeepLabCut.git cloned-DLC-repo\n","%cd cloned-DLC-repo\n","\n","from IPython.display import clear_output\n","!pip install deeplabcut\n","clear_output()\n","\n","import os\n","# os.kill(os.getpid(), 9)     # Comment this line out after first run"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgZ0SD5BNtC0","colab_type":"code","outputId":"b1d2b206-8ed3-41e8-f4f5-586313bbc9f5","executionInfo":{"status":"ok","timestamp":1570562775784,"user_tz":-120,"elapsed":14462,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"02239620675032514891"}},"colab":{"base_uri":"https://localhost:8080/","height":219}},"source":["# Environment setup \n","\n","# GUIs don't work on the cloud, so we will supress wxPython: \n","%cd /content/cloned-DLC-repo\n","os.environ[\"DLClight\"]=\"True\"\n","os.environ[\"Colab\"]=\"True\"\n","\n","import deeplabcut\n","\n","# Create a path variable that links to the config file:\n","from pathlib import Path\n","path_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/config_colab.yaml'\n","path_pose_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/dlc-models/iteration-4/CheetahApr2-trainset95shuffle1/train/pose_cfg_colab.yaml'\n","path_extension = str(Path(path_pose_config_file).parents[4] / 'extension-models' / Path(path_pose_config_file).parents[2].stem / Path(path_pose_config_file).parents[1].stem / Path(path_pose_config_file).parents[0].stem / 'Batch8')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n","Project loaded in colab-mode. Apparently Colab has trouble loading statsmodels, so the smoothing & outlier frame extraction is disabled. Sorry!\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","DLC loaded in light mode; you cannot use the labeling GUI!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"awyiNojxlbzB","colab_type":"text"},"source":["# Score Map Generation"]},{"cell_type":"markdown","metadata":{"id":"ClDkZdBfm3MF","colab_type":"text"},"source":["### analyze_videos"]},{"cell_type":"code","metadata":{"id":"sV5W_CjmmNlB","colab_type":"code","colab":{}},"source":["# Adapted from DLC analyze_videos\n","# def analyze_videos(config,videos,videotype='avi',shuffle=1,trainingsetindex=0,gputouse=None,save_as_csv=False, destfolder=None,cropping=None): #debug\n","# analyze_videos(path_config_file,videofile_path, videotype='.mp4') #debug\n","\n","%cd /content/cloned-DLC-repo\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","from deeplabcut.pose_estimation_tensorflow.config import load_config\n","from deeplabcut.pose_estimation_tensorflow.dataset.factory import create as create_dataset\n","from deeplabcut.pose_estimation_tensorflow.nnet import predict\n","from deeplabcut.utils import auxiliaryfunctions\n","\n","##################################################\n","# Parameter Defaults from function definition\n","##################################################\n","\n","shuffle=1\n","trainingsetindex=0\n","gputouse=None\n","save_as_csv=False\n","destfolder=None\n","cropping=None\n","\n","##################################################\n","# Adapted Function\n","##################################################\n","\n","if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:\n","    del os.environ['TF_CUDNN_USE_AUTOTUNE'] #was potentially set during training\n","\n","if gputouse is not None: #gpu selection\n","        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)\n","\n","vers = (tf.__version__).split('.')\n","if int(vers[0])==1 and int(vers[1])>12:\n","    TF=tf.compat.v1\n","else:\n","    TF=tf\n","\n","TF.reset_default_graph()\n","start_path=os.getcwd() #record cwd to return to this directory in the end\n","\n","cfg = auxiliaryfunctions.read_config(path_config_file) #JO\n","\n","if cropping is not None:\n","    cfg['cropping']=True\n","    cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping\n","    print(\"Overwriting cropping parameters:\", cropping)\n","    print(\"These are used for all videos, but won't be save to the cfg file.\")\n","\n","trainFraction = cfg['TrainingFraction'][trainingsetindex]\n","\n","modelfolder=os.path.join(cfg[\"project_path\"],str(auxiliaryfunctions.GetModelFolder(trainFraction,shuffle,cfg)))   #improvement, no need from these lines\n","path_train_config = Path(modelfolder) / 'train' / 'pose_cfg_colab.yaml'   #JO x2 #PSE-test->train\n","try:\n","    dlc_cfg = load_config(str(path_train_config))\n","except FileNotFoundError:\n","    raise FileNotFoundError(\"It seems the model for shuffle %s and trainFraction %s does not exist.\"%(shuffle,trainFraction))\n","\n","# Check which snapshots are available and sort them by # iterations\n","try:\n","  Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(os.path.join(modelfolder , 'train'))if \"index\" in fn])\n","except FileNotFoundError:\n","  raise FileNotFoundError(\"Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\\n Please train it before using it to analyze videos.\\n Use the function 'train_network' to train the network for shuffle %s.\"%(shuffle,shuffle))\n","\n","if cfg['snapshotindex'] == 'all':\n","    print(\"Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!\")\n","    snapshotindex = -1\n","else:\n","    snapshotindex=cfg['snapshotindex']\n","\n","increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","Snapshots = Snapshots[increasing_indices]\n","\n","print(\"Using %s\" % Snapshots[snapshotindex], \"for model\", modelfolder)\n","\n","##################################################\n","# Load and setup CNN part detector\n","##################################################\n","\n","# Check if data already was generated:\n","dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])  #useful for later (u4l)\n","trainingsiterations = (dlc_cfg['init_weights'].split(os.sep)[-1]).split('-')[-1]\n","\n","#update batchsize (based on parameters in config.yaml)\n","dlc_cfg['batch_size']=cfg['batch_size']\n","\n","# update number of outputs\n","dlc_cfg['num_outputs'] = cfg.get('num_outputs', 1)\n","\n","print('num_outputs = ', dlc_cfg['num_outputs'])\n","\n","# Name for scorer:\n","DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations=trainingsiterations)\n","\n","sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)  #u4l-How to get predictions from model   #PSE - tf.sigmoid applyed to part-predict \n","\n","xyz_labs_orig = ['x', 'y', 'likelihood']\n","suffix = [str(s+1) for s in range(dlc_cfg['num_outputs'])]\n","suffix[0] = '' # first one has empty suffix for backwards compatibility\n","xyz_labs = [x+s for s in suffix for x in xyz_labs_orig]\n","\n","pdindex = pd.MultiIndex.from_product([[DLCscorer],\n","                                      dlc_cfg['all_joints_names'],\n","                                      xyz_labs],\n","                                      names=['scorer', 'bodyparts', 'coords'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q80tr9PNE6Qr","colab_type":"text"},"source":["### Generate and Save Scoremaps"]},{"cell_type":"code","metadata":{"id":"kiAE_I_MXr7t","colab_type":"code","colab":{}},"source":["# # has code tp protect against repeated analysis but is\n","# # commented to prevent colab trying to access 2000+ files which is slow \n","\n","# %cd '/content/cloned-DLC-repo'\n","\n","# dataset = create_dataset(dlc_cfg)\n","\n","# print(\"Starting to extract scoremaps & locrefs\")\n","\n","# %cd $dlc_cfg.project_path\n","        \n","# from scipy.misc import imread\n","# from tqdm import tqdm\n","\n","# nframes = len(dataset.data)\n","# pbar=tqdm(total=nframes)\n","# step=max(10,int(nframes/100)) \n","# counter = 0\n","# batch_counter = 1\n","# images = {}\n","\n","# # for DataItem in (dataset.data):\n","# for index in range(nframes):\n","#   if counter%step==0:\n","#     pbar.update(step)\n","\n","#   if counter % 150 == 0 and counter != 0:\n","#       np.savez('batched-data/Image_Batch_'+str(batch_counter),**images)\n","#       batch_counter += 1\n","#       images = {}\n","\n","#   # Adapted from DLC GetPoseS  \n","#   image = imread(os.path.join(dataset.data[index].im_path), mode='RGB')\n","#   images[str(dataset.data[index].im_path)] = image\n","\n","#   counter += 1\n","\n","# print('Finished for loop with {} images remaining'.format(counter))\n","# # np.savez('Image_Batch_End',key=value for key,value in images.items(),allow_pickle=False,fix_imports=False)\n","# np.savez('batched-data/Image_Batch_'+str(batch_counter),**images)\n","\n","\n","# pbar.close()\n","# # clear_output()\n","# %cd /content/cloned-DLC-repo\n","\n","# sess.close()\n","# TF.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrJIGvVm40oq","colab_type":"code","colab":{}},"source":["# %cd $dlc_cfg.project_path\n","# t_batch1 = np.load('Image_Batch_150.npz')\n","# %cd /content/cloned-DLC-repo\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uyiZnvt6RCuh"},"source":["### Batch Save Scoremaps"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n3QU4rNlRCuj","colab":{}},"source":["# # has code tp protect against repeated analysis but is\n","# # commented to prevent colab trying to access 2000+ files which is slow \n","\n","# %cd '/content/cloned-DLC-repo'\n","\n","# list_imgs = np.load('../drive/Shared drives/Final Year Project/Datasets/list_images_without_scoremaps' + '.npy', mmap_mode=None, allow_pickle=False, fix_imports=False)\n","# # print(list_imgs)\n","# print(\"Starting to extract scoremaps & locrefs\")\n","\n","# %cd $dlc_cfg.project_path\n","\n","# import cv2\n","# from tqdm import tqdm\n","# from skimage.util import img_as_ubyte\n","\n","# nframes = len(list_imgs)\n","# pbar=tqdm(total=nframes)\n","# step=max(10,int(nframes/100)) \n","# counter = 0\n","\n","# for img_path in (list_imgs):\n","#   save_name = str(Path(img_path).parents[0] / ('eval_' + Path(img_path).stem))\n","\n","#   counter += 1\n","#   if counter%step==0:\n","#     pbar.update(step)\n","\n","#   try:\n","#     # Attempt to load scoremap data\n","#     np.load(save_name + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)\n","#     print(\"Scoremaps for {} already extracted!\".format(save_name))\n","#   except FileNotFoundError:\n","#     # Adapted from DLC GetPoseS  \n","#     image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","#     image = img_as_ubyte(image)\n","\n","#     # Adapted DLC predict.getpose\n","#     im=np.expand_dims(image, axis=0).astype(float)\n","#     outputs_np = sess.run(outputs, feed_dict={inputs: im})  \n","#     scmap, locref = predict.extract_cnn_output(outputs_np, dlc_cfg)    \n","\n","#     np.savez(save_name,scmap=scmap,locref=locref,allow_pickle=False,fix_imports=False)\n","#     print(\"Image {} analyzed!\".format(save_name))\n","\n","# pbar.close()\n","# # clear_output()\n","# %cd /content/cloned-DLC-repo\n","\n","# sess.close()\n","# TF.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_WTE2Al1RCul","colab":{}},"source":["# testing to delete\n","\n","#np.load(save_name + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)\n","\n","\n","# import cv2\n","# from deeplabcut.pose_estimation_tensorflow.dataset.factory import create as create_dataset\n","\n","# dataset = create_dataset(dlc_cfg)\n","\n","# test_str = (dlc_cfg.project_path+'/'+dataset.data[0].im_path)\n","# print(test_str)\n","\n","# test_img = cv2.imread(test_str)\n","# print(type(test_img))\n","\n","# code = cv2.COLOR_BGR2RGB\n","# cvt = cv2.cvtColor(test_img,code)\n","\n","# print(type(cvt))\n"],"execution_count":0,"outputs":[]}]}