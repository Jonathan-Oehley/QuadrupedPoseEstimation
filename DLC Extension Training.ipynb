{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DLC Extension Runtime 11.ipynb","provenance":[{"file_id":"1j5PEwaOVvT13Fq3elGy3JYSlp2EReBSF","timestamp":1570287311631}],"collapsed_sections":["w9zKBC2WMk47","0FgtyiwnMtjs","awyiNojxlbzB","UIqJw3HVltkv"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"w9zKBC2WMk47","colab_type":"text"},"source":["## Setup Code"]},{"cell_type":"markdown","metadata":{"id":"-1wCJICbsZ3p","colab_type":"text"},"source":["### Drive Setup"]},{"cell_type":"code","metadata":{"id":"_vCN9I7Gto0q","colab_type":"code","outputId":"f0373a45-2c84-482c-b5aa-b42993e2e996","executionInfo":{"status":"ok","timestamp":1570966922039,"user_tz":-120,"elapsed":9631,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"18240730775851385519"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0FgtyiwnMtjs","colab_type":"text"},"source":["### DLC Setup\n","Code created to automatically crash the notebook to reload the dependencies that were imported. Requires manual comment of os.kill line of code after the first run"]},{"cell_type":"code","metadata":{"id":"wMp8pFAGMvSR","colab_type":"code","colab":{}},"source":["# Download and installation\n","%cd /content\n","!git clone -l -s git://github.com/AlexEMG/DeepLabCut.git cloned-DLC-repo\n","%cd cloned-DLC-repo\n","\n","from IPython.display import clear_output\n","# !pip install deeplabcut\n","clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LulCX7utwsyQ","colab_type":"text"},"source":["#### Setup.py write"]},{"cell_type":"code","metadata":{"id":"u05OxbjKOmA6","colab_type":"code","outputId":"b6ad5d78-e87b-40ef-d978-56e8063e4875","executionInfo":{"status":"ok","timestamp":1570966925431,"user_tz":-120,"elapsed":12991,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"18240730775851385519"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["%%writefile setup.py\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","DeepLabCut2.0 Toolbox (deeplabcut.org)\n","Â© A. & M. Mathis Labs\n","https://github.com/AlexEMG/DeepLabCut\n","\n","Please see AUTHORS for contributors.\n","https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS\n","Licensed under GNU Lesser General Public License v3.0\n","\"\"\"\n","\n","import setuptools\n","\n","with open(\"README.md\", \"r\") as fh:\n","    long_description = fh.read()\n","\n","setuptools.setup(\n","    name=\"deeplabcut\",\n","    version=\"2.0.9\",\n","    author=\"A. & M. Mathis Labs\",\n","    author_email=\"alexander.mathis@bethgelab.org\",\n","    description=\"Markerless pose-estimation of user-defined features with deep learning\",\n","    long_description=long_description,\n","    long_description_content_type=\"text/markdown\",\n","    url=\"https://github.com/AlexEMG/DeepLabCut\",\n","    install_requires=['certifi','chardet~=3.0.4','click','easydict~=1.7',\n","                      'gast==0.2.2','h5py~=2.7','imageio~=2.3.0','intel-openmp',\n","                      'ipython~=6.0.0','ipython-genutils~=0.2.0',\n","                      'matplotlib~=3.0.3','moviepy~=0.2.3.5','numpy~=1.14.5','opencv-python~=3.4',\n","                      'pandas>=0.21.0','patsy','python-dateutil~=2.7.3','pyyaml>=5.1','requests',\n","                      'ruamel.yaml~=0.15','setuptools','scikit-image~=0.14.0','scikit-learn~=0.19.2',\n","                      'scipy~=1.1.0','statsmodels~=0.9.0','tables',\n","                      'tensorpack~=0.9.7.1',\n","                      'tqdm>4.29','wheel~=0.31.1'],\n","    scripts=['deeplabcut/pose_estimation_tensorflow/models/pretrained/download.sh'],\n","    packages=setuptools.find_packages(),\n","    data_files=[('deeplabcut',['deeplabcut/pose_cfg.yaml','deeplabcut/pose_estimation_tensorflow/models/pretrained/pretrained_model_urls.yaml'])],\n","    include_package_data=True,\n","    classifiers=(\n","        \"Programming Language :: Python :: 3\",\n","        \"License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)\",\n","        \"Operating System :: OS Independent\",\n","    ),\n","    entry_points=\"\"\"[console_scripts]\n","            dlc=dlc:main\"\"\",\n",")\n","\n","#https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting setup.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2eJCk-oMw0NP","colab_type":"text"},"source":["### Remaining Setup"]},{"cell_type":"code","metadata":{"id":"tDypMZZbw5L1","colab_type":"code","colab":{}},"source":["%cd /content\n","!pip install -e cloned-DLC-repo\n","clear_output()\n","\n","import os\n","# os.kill(os.getpid(), 9)     # Comment this line out after first run\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgZ0SD5BNtC0","colab_type":"code","outputId":"eff4e506-df64-420c-82f3-d93c36eb3ddb","executionInfo":{"status":"ok","timestamp":1570966936676,"user_tz":-120,"elapsed":24201,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"18240730775851385519"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["# Environment setup \n","\n","# GUIs don't work on the cloud, so we will supress wxPython: \n","%cd /content/cloned-DLC-repo\n","os.environ[\"DLClight\"]=\"True\"\n","os.environ[\"Colab\"]=\"True\"\n","\n","import deeplabcut\n","\n","# Create a path variable that links to the config file:\n","from pathlib import Path\n","path_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/config_colab.yaml'\n","path_pose_config_file = '/content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/dlc-models/iteration-4/CheetahApr2-trainset95shuffle1/train/pose_cfg_colab.yaml'\n","model_version =  'ImgReworkDS'\n","snapshot_name = 'IRWK_DS_snapshot'\n","path_extension = str(Path(path_pose_config_file).parents[4] / 'extension-models' / Path(path_pose_config_file).parents[2].stem / Path(path_pose_config_file).parents[1].stem / Path(path_pose_config_file).parents[0].stem / model_version)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n","Project loaded in colab-mode. Apparently Colab has trouble loading statsmodels, so the smoothing & outlier frame extraction is disabled. Sorry!\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","DLC loaded in light mode; you cannot use the labeling GUI!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"awyiNojxlbzB","colab_type":"text"},"source":["## Score Map Generation"]},{"cell_type":"markdown","metadata":{"id":"ClDkZdBfm3MF","colab_type":"text"},"source":["### analyze_videos"]},{"cell_type":"code","metadata":{"id":"sV5W_CjmmNlB","colab_type":"code","outputId":"0809c99c-fcb7-405d-c3e2-a0def73e1bd6","executionInfo":{"status":"ok","timestamp":1570966958166,"user_tz":-120,"elapsed":45679,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"18240730775851385519"}},"colab":{"base_uri":"https://localhost:8080/","height":309}},"source":["# Adapted from DLC analyze_videos\n","# def analyze_videos(config,videos,videotype='avi',shuffle=1,trainingsetindex=0,gputouse=None,save_as_csv=False, destfolder=None,cropping=None): #debug\n","# analyze_videos(path_config_file,videofile_path, videotype='.mp4') #debug\n","\n","%cd /content/cloned-DLC-repo\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","from deeplabcut.pose_estimation_tensorflow.config import load_config\n","from deeplabcut.pose_estimation_tensorflow.dataset.factory import create as create_dataset\n","from deeplabcut.pose_estimation_tensorflow.nnet import predict\n","from deeplabcut.utils import auxiliaryfunctions\n","\n","##################################################\n","# Parameter Defaults from function definition\n","##################################################\n","\n","shuffle=1\n","trainingsetindex=0\n","gputouse=None\n","save_as_csv=False\n","destfolder=None\n","cropping=None\n","\n","##################################################\n","# Adapted Function\n","##################################################\n","\n","if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:\n","    del os.environ['TF_CUDNN_USE_AUTOTUNE'] #was potentially set during training\n","\n","if gputouse is not None: #gpu selection\n","        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)\n","\n","vers = (tf.__version__).split('.')\n","if int(vers[0])==1 and int(vers[1])>12:\n","    TF=tf.compat.v1\n","else:\n","    TF=tf\n","\n","TF.reset_default_graph()\n","start_path=os.getcwd() #record cwd to return to this directory in the end\n","\n","cfg = auxiliaryfunctions.read_config(path_config_file) #JO\n","\n","if cropping is not None:\n","    cfg['cropping']=True\n","    cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping\n","    print(\"Overwriting cropping parameters:\", cropping)\n","    print(\"These are used for all videos, but won't be save to the cfg file.\")\n","\n","trainFraction = cfg['TrainingFraction'][trainingsetindex]\n","\n","modelfolder=os.path.join(cfg[\"project_path\"],str(auxiliaryfunctions.GetModelFolder(trainFraction,shuffle,cfg)))   #improvement, no need from these lines\n","path_train_config = Path(modelfolder) / 'train' / 'pose_cfg_colab.yaml'   #JO x2 -test->train\n","try:\n","    dlc_cfg = load_config(str(path_train_config))\n","except FileNotFoundError:\n","    raise FileNotFoundError(\"It seems the model for shuffle %s and trainFraction %s does not exist.\"%(shuffle,trainFraction))\n","\n","# Check which snapshots are available and sort them by # iterations\n","try:\n","  Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(os.path.join(modelfolder , 'train'))if \"index\" in fn])\n","except FileNotFoundError:\n","  raise FileNotFoundError(\"Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\\n Please train it before using it to analyze videos.\\n Use the function 'train_network' to train the network for shuffle %s.\"%(shuffle,shuffle))\n","\n","if cfg['snapshotindex'] == 'all':\n","    print(\"Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!\")\n","    snapshotindex = -1\n","else:\n","    snapshotindex=cfg['snapshotindex']\n","\n","increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","Snapshots = Snapshots[increasing_indices]\n","\n","print(\"Using %s\" % Snapshots[snapshotindex], \"for model\", modelfolder)\n","\n","##################################################\n","# Load and setup CNN part detector\n","##################################################\n","\n","# Check if data already was generated:\n","dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])  #useful for later (u4l)\n","trainingsiterations = (dlc_cfg['init_weights'].split(os.sep)[-1]).split('-')[-1]\n","\n","#update batchsize (based on parameters in config.yaml)\n","dlc_cfg['batch_size']=cfg['batch_size']\n","\n","# update number of outputs\n","dlc_cfg['num_outputs'] = cfg.get('num_outputs', 1)\n","\n","print('num_outputs = ', dlc_cfg['num_outputs'])\n","\n","# Name for scorer:\n","DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations=trainingsiterations)\n","\n","sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)  #u4l-How to get predictions from model   #PSE - tf.sigmoid applyed to part-predict \n","\n","xyz_labs_orig = ['x', 'y', 'likelihood']\n","suffix = [str(s+1) for s in range(dlc_cfg['num_outputs'])]\n","suffix[0] = '' # first one has empty suffix for backwards compatibility\n","xyz_labs = [x+s for s in suffix for x in xyz_labs_orig]\n","\n","pdindex = pd.MultiIndex.from_product([[DLCscorer],\n","                                      dlc_cfg['all_joints_names'],\n","                                      xyz_labs],\n","                                      names=['scorer', 'bodyparts', 'coords'])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n","Using snapshot-1000000 for model /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/dlc-models/iteration-4/CheetahApr2-trainset95shuffle1\n","num_outputs =  1\n","Initializing ResNet\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /content/cloned-DLC-repo/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py:75: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /content/cloned-DLC-repo/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py:75: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/dlc-models/iteration-4/CheetahApr2-trainset95shuffle1/train/snapshot-1000000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/Shared drives/Final Year Project/Datasets/Cheetah-AnChi-2019-04-02/dlc-models/iteration-4/CheetahApr2-trainset95shuffle1/train/snapshot-1000000\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"UIqJw3HVltkv","colab_type":"text"},"source":["# Regression Subnetwork\n","Data loading -> modfying sub functions of load_and_enqueue \n","\n","Model Definition \n","\n","-> structure \n","\n","-> Loss function"]},{"cell_type":"markdown","metadata":{"id":"zOb40ZgBl4b8","colab_type":"text"},"source":["## train_network"]},{"cell_type":"code","metadata":{"id":"yMD90rxGmQTR","colab_type":"code","outputId":"ada6803f-9f0e-4c08-8998-fad94d5f24e3","executionInfo":{"status":"ok","timestamp":1570966958171,"user_tz":-120,"elapsed":45648,"user":{"displayName":"Jonathan Oehley","photoUrl":"","userId":"18240730775851385519"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Adapted from DLC train_network\n","# def train_network(config,shuffle=1,trainingsetindex=0,gputouse=None,max_snapshots_to_keep=5,autotune=False,displayiters=None,saveiters=None,maxiters=None): #debug\n","# deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=100) #debug\n","\n","%cd /content/cloned-DLC-repo\n","\n","##################################################\n","# Parameter Defaults from function definition\n","##################################################\n","\n","# Definition\n","max_snapshots_to_keep=10\n","autotune=False\n","# moved iters parameters to train\n","\n","##################################################\n","# Adapted Function\n","##################################################\n","\n","import importlib\n","import logging\n","importlib.reload(logging)\n","logging.shutdown()\n","\n","TF.reset_default_graph()\n","\n","if autotune is not False: #see: https://github.com/tensorflow/tensorflow/issues/13317\n","    os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/cloned-DLC-repo\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PF1vhmoBnmA9","colab_type":"text"},"source":["## Model Definition "]},{"cell_type":"markdown","metadata":{"id":"ib1jEnU7yoak","colab_type":"text"},"source":["### Bottleneck Module Definition\n","\n"]},{"cell_type":"code","metadata":{"id":"v2H2plJ2ynwq","colab_type":"code","colab":{}},"source":["from tensorflow.contrib import layers\n","from tensorflow.contrib.framework.python.ops import add_arg_scope\n","from tensorflow.contrib.layers.python.layers import utils\n","from tensorflow.contrib.slim.python.slim.nets import resnet_utils\n","from tensorflow.python.ops import nn_ops\n","from tensorflow.python.ops import variable_scope\n","\n","@add_arg_scope\n","def bottleneck(inputs,\n","               depth,\n","               depth_bottleneck,\n","               stride,\n","               rate=1,\n","               outputs_collections=None,\n","               scope=None):\n","  \"\"\"Bottleneck residual unit variant with BN after convolutions.\n","  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n","  its definition. Note that we use here the bottleneck variant which has an\n","  extra bottleneck layer.\n","  When putting together two consecutive ResNet blocks that use this unit, one\n","  should use stride = 2 in the last unit of the first block.\n","  Args:\n","    inputs: A tensor of size [batch, height, width, channels].\n","    depth: The depth of the ResNet unit output.\n","    depth_bottleneck: The depth of the bottleneck layers.\n","    stride: The ResNet unit's stride. Determines the amount of downsampling of\n","      the units output compared to its input.\n","    rate: An integer, rate for atrous convolution.\n","    outputs_collections: Collection to add the ResNet unit output.\n","    scope: Optional variable_scope.\n","  Returns:\n","    The ResNet unit's output.\n","  \"\"\"\n","  with variable_scope.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:\n","    depth_in = utils.last_dimension(inputs.get_shape(), min_rank=4)\n","    if depth == depth_in:   #PSE\n","      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n","    else:\n","        shortcut = layers.conv2d(\n","            inputs,\n","            depth, [1, 1],\n","            stride=stride,\n","            activation_fn=None,\n","            scope='shortcut')\n","    residual = layers.conv2d(\n","        inputs, depth_bottleneck, [1, 1], stride=1, scope='conv1')\n","    residual = resnet_utils.conv2d_same(\n","        residual, depth_bottleneck, 3, stride, rate=rate, scope='conv2')\n","    residual = layers.conv2d(\n","        residual, depth, [1, 1], stride=1, activation_fn=None, scope='conv3')\n","\n","    output = nn_ops.relu(shortcut + residual)\n","\n","    return utils.collect_named_outputs(outputs_collections, sc.name, output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbSomToBwj5W","colab_type":"text"},"source":["### Regnet Definition and Help functions"]},{"cell_type":"code","metadata":{"id":"h19_DM4yzyvc","colab_type":"code","colab":{}},"source":["# Helper functions for regnet\n","from tensorflow.contrib.layers.python.layers import layers as layers_lib\n","\n","def regnet_block_1(inputs,scope='regnet_block_1_'):\n","  net = layers.conv2d(\n","      inputs, 64, 7, stride=2,padding='SAME', scope=scope+'conv1')\n","  net = layers_lib.max_pool2d(\n","      net, [2, 2], stride=2, padding='SAME', scope=scope+'pool1')\n","  \n","  return net\n","\n","def regnet_block_2(inputs,\n","               depth_bottleneck,\n","               depth=256,\n","               stride=1,\n","               rate=1,\n","               scope=None):\n","  \n","  # Three bottleneck modules\n","  # Scope passed to function should include instance number\n","  if scope == None:\n","    net = bottleneck(\n","        inputs,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","  else:\n","    net = bottleneck(\n","        inputs,depth,depth_bottleneck,stride,rate,scope=scope+'_1')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_2')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_3')\n","\n","  return  net\n","\n","def regnet_block_3(inputs,\n","               depth_bottleneck,\n","               depth=256,\n","               stride=1,\n","               rate=1,\n","               scope=None):\n","  \n","  # Three bottleneck modules preceded by a maxpooling layer\n","  # Scope passed to function should include instance number\n","\n","  if scope == None:\n","    net = layers.max_pool2d(\n","        inputs, [2, 2], stride=2, padding='SAME')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate)     \n","  else:\n","    net = layers.max_pool2d(\n","        inputs, [2, 2], stride=2, padding='SAME', scope=scope+'pool')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_1')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_2')\n","    net = bottleneck(\n","        net,depth,depth_bottleneck,stride,rate,scope=scope+'_3')    \n","\n","  return net\n","\n","def regnet_block_4(inputs,\n","               depth_bottleneck,\n","               scope=None):\n","  \n","  # Helper function to pass straight to regnet_block_2\n","  # Assists in scope handling\n","  net = regnet_block_2(\n","      inputs, depth_bottleneck=depth_bottleneck, scope=scope)\n","\n","  return net\n","\n","def regnet_block_5(inputs,\n","               depth,\n","               scope=None):\n","  \n","  net = layers.conv2d_transpose(\n","      inputs, depth, [2, 2], stride=2, padding='SAME', scope=scope)   \n","\n","  return net\n","\n","def regnet_block_6(inputs,\n","               depth,\n","               scope=None):\n","\n","  net = layers.conv2d(\n","        inputs, depth, [1, 1], stride=1, activation_fn=None, normalizer_fn=None, scope=scope, padding='SAME')   #toMod, activation and norm functions?\n","  \n","  return net\n","\n","def regnet_block_7(inputs,\n","               num_outputs,\n","               scope=None):\n","  \n","  net = layers.conv2d(                                                                              \n","        inputs, num_outputs, [1, 1], stride=1, activation_fn=None, normalizer_fn=None, scope=scope, padding='SAME')   #toMod, activation and norm functions?\n","\n","  return net\n","\n","def regnet_block_8(inputs,\n","               num_outputs,\n","               scope=None):\n","\n","  net = layers.conv2d_transpose(\n","    inputs, num_outputs, [2, 2], stride=2, scope=scope, padding='SAME')\n","  \n","  return net  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgykOG-RH5MU","colab_type":"code","colab":{}},"source":["# Hourglass construction helper function\n","def hourglass(inputs, hg_depth, scope='HG_'):\n","  assert hg_depth >= 1\n","  hg_level = '_'+str(hg_depth)\n","\n","  net = regnet_block_3(inputs, depth_bottleneck=128, scope=scope+'regnet_block_3'+hg_level)\n","\n","  if hg_depth == 1: \n","    shortcut = regnet_block_5(net, depth=256, scope=scope+'regnet_block_5'+hg_level)\n","  else:\n","    shortcut = regnet_block_5(hourglass(net,hg_depth-1,scope=scope), depth=256, scope=scope+'regnet_block_5'+hg_level)\n","\n","  residual = regnet_block_4(inputs, depth_bottleneck=128, scope=scope+'regnet_block_4'+hg_level)\n","\n","  return residual + shortcut     \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ioyzv0pywqj0","colab_type":"code","colab":{}},"source":["# Regnet function definition\n","\n","def regnet(inputs, scmap_inputs, num_outputs):\n","  \n","  inputs = regnet_block_1(inputs)  \n","\n","  stacked_input = tf.concat([inputs,scmap_inputs],-1)\n","  # stacked_input = inputs\n","\n","  net = regnet_block_2(stacked_input, depth_bottleneck=64, scope='regnet_block_2_1')\n","  net = hourglass(net, hg_depth=4,scope='HG1_')\n","  net = regnet_block_6(net, depth=512, scope='regnet_block_6_1')\n","  net = regnet_block_6(net, depth=512, scope='regnet_block_6_2')\n","  net = regnet_block_7(net, num_outputs=num_outputs, scope='regnet_block_7_1')  \n","  # net = regnet_block_8(net, num_outputs=num_outputs, scope='regnet_block_8_1')  #Comment out for half scale, uncomment for full scale\n","\n","  return net \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8eqKf6jTEjr","colab_type":"text"},"source":["## train Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"8Rz4TeuYUxke","colab_type":"text"},"source":["### Batch Class Definition"]},{"cell_type":"code","metadata":{"id":"WCH-I9ZKU05K","colab_type":"code","colab":{}},"source":["# Adapted from DeeperCut pose_dataset.py definition\n","from enum import Enum\n","\n","class Batch(Enum):\n","    inputs = 0\n","    part_score_targets = 1\n","    part_score_weights = 2\n","    locref_targets = 3\n","    locref_mask = 4\n","    pairwise_targets = 5\n","    pairwise_mask = 6\n","    data_item = 7\n","    scmap_inputs = 8"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gp6YUfEVO9a7","colab_type":"text"},"source":["### pose_net function definitions\n","Functions of the same name adapted from DeeperCut pose_net.py"]},{"cell_type":"markdown","metadata":{"id":"EKH0WX4alQL0","colab_type":"text"},"source":["#### get_batch_spec"]},{"cell_type":"code","metadata":{"id":"mIKnZZD3P4Nm","colab_type":"code","colab":{}},"source":["# get_batch_spec() function\n","\n","def get_batch_spec(cfg):\n","    num_joints = cfg.num_joints\n","    batch_size = cfg.batch_size\n","    scoremap_and_locref_channel_multiplier = 3\n","    return {  \n","        Batch.inputs: [batch_size, None, None, 3],\n","        Batch.part_score_targets: [batch_size, None, None, num_joints],\n","        Batch.part_score_weights: [batch_size, None, None, num_joints],\n","        Batch.locref_targets: [batch_size, None, None, num_joints * 2],\n","        Batch.locref_mask: [batch_size, None, None, num_joints * 2],\n","        Batch.scmap_inputs: [batch_size, None, None, num_joints * scoremap_and_locref_channel_multiplier]\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6gB4Zx7CcvNx","colab_type":"text"},"source":["#### prediction_layer"]},{"cell_type":"code","metadata":{"id":"0LVyEd7Tc0ov","colab_type":"code","colab":{}},"source":["# prediction_layer() function\n","#toMod -> should scope arg be changed here? -> happy with their layer hyper-parameters?\n","\n","import tensorflow.contrib.slim as slim\n","\n","def prediction_layer(cfg, input, name, num_outputs):\n","    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME',\n","                        activation_fn=None, normalizer_fn=None,\n","                        weights_regularizer=slim.l2_regularizer(cfg.weight_decay)):\n","        with TF.variable_scope(name):\n","            pred = layers.conv2d_transpose(input, num_outputs,            # changed slim to layers\n","                                         kernel_size=[2, 2], stride=2,    #adapted kernal stride\n","                                         scope='regnet_pred_layers', padding='SAME')    #inserted padding for sanity check\n","            return pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RdU-YvAlUYH","colab_type":"text"},"source":["#### PoseNet Class Definition"]},{"cell_type":"code","metadata":{"id":"tNWL38kMkg1f","colab_type":"code","colab":{}},"source":["# PoseNet Class\n","\n","from tensorflow.contrib.slim.python.slim.nets.resnet_utils import resnet_arg_scope\n","\n","class PoseNet:\n","    def __init__(self, dlc_cfg):\n","        self.dlc_cfg = dlc_cfg\n","\n","    def extract_features(self, inputs, scmap_inputs):\n","        mean = tf.constant(self.dlc_cfg.mean_pixel,\n","                           dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean') \n","        im_centered = inputs - mean             #toMod, PSE, -> what is this doing? What should it be? Should I do the same with scmap_inputs\n","\n","        # The next part of the code depends upon which tensorflow version you have.\n","        vers = tf.__version__\n","        vers = vers.split(\".\") #Updated based on https://github.com/AlexEMG/DeepLabCut/issues/44\n","        if int(vers[0])==1 and int(vers[1])<4: #check if lower than version 1.4.\n","            with slim.arg_scope(resnet_arg_scope(False)):\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints)\n","        else:\n","            with slim.arg_scope(resnet_arg_scope()):\n","                net = regnet(im_centered, scmap_inputs, self.dlc_cfg.num_joints)\n","\n","        return net\n","\n","    def prediction_layers(self, features, reuse=None):\n","        dlc_cfg = self.dlc_cfg\n","\n","        out = {}\n","        with TF.variable_scope('pose', reuse=reuse):\n","            out['part_pred'] = prediction_layer(dlc_cfg, features, 'part_pred',\n","                                                dlc_cfg.num_joints)\n","            if dlc_cfg.location_refinement:\n","                out['locref'] = prediction_layer(dlc_cfg, features, 'locref_pred',\n","                                                 dlc_cfg.num_joints * 2)\n","    \n","        return out\n","\n","    def get_net(self, inputs, scmap_inputs):\n","        net = self.extract_features(inputs, scmap_inputs)\n","        return self.prediction_layers(net)\n","\n","    def test(self, inputs, scmap_inputs):\n","        heads = self.get_net(inputs, scmap_inputs)\n","        prob = tf.sigmoid(heads['part_pred'])\n","        return {'part_prob': prob, 'locref': heads['locref']}\n","\n","    def train(self, batch):\n","        dlc_cfg = self.dlc_cfg\n","\n","        if dlc_cfg.deterministic:\n","            tf.set_random_seed(42)\n","\n","        heads = self.get_net(batch[Batch.inputs], batch[Batch.scmap_inputs])\n","\n","        weigh_part_predictions = dlc_cfg.weigh_part_predictions\n","        part_score_weights = batch[Batch.part_score_weights] if weigh_part_predictions else 1.0\n","\n","        def add_part_loss(pred_layer):\n","            return TF.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets],\n","                                                   heads[pred_layer],\n","                                                   part_score_weights)\n","\n","        loss = {}\n","        loss['part_loss'] = add_part_loss('part_pred')\n","        total_loss = loss['part_loss']\n","\n","        if dlc_cfg.location_refinement:\n","            locref_pred = heads['locref']\n","            locref_targets = batch[Batch.locref_targets]\n","            locref_weights = batch[Batch.locref_mask]\n","\n","            loss_func = losses.huber_loss if dlc_cfg.locref_huber_loss else tf.losses.mean_squared_error\n","            loss['locref_loss'] = dlc_cfg.locref_loss_weight * loss_func(locref_targets, locref_pred, locref_weights)\n","            total_loss = total_loss + loss['locref_loss']\n","\n","    #     # loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize)\n","        loss['total_loss'] = total_loss\n","        return loss\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56EHOD1nX4dS","colab_type":"text"},"source":["### Learning Rate Object"]},{"cell_type":"code","metadata":{"id":"bU7DUll6X8rd","colab_type":"code","colab":{}},"source":["class LearningRate(object):\n","    def __init__(self, dlc_cfg, trainediters):\n","        self.steps = dlc_cfg.multi_step\n","        self.current_step = 0\n","        if trainediters != -1:\n","          for it in range(trainediters):\n","            if it == self.steps[self.current_step][1]:\n","              self.current_step += 1\n","\n","    def get_lr(self, iteration):\n","        lr = self.steps[self.current_step][0]\n","        if iteration == self.steps[self.current_step][1]:\n","            self.current_step += 1\n","\n","        return lr\n","\n","#Current multistep setting\n","# - - 0.005\n","#   - 10000\n","# - - 0.02\n","#   - 430000\n","# - - 0.002\n","#   - 730000\n","# - - 0.001\n","#   - 1030000"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_tXZogHd43lr","colab_type":"text"},"source":["### PoseDataset Class Definition"]},{"cell_type":"code","metadata":{"id":"TB7qzyj15Hca","colab_type":"code","colab":{}},"source":["import random as rand\n","import scipy.io as sio\n","from scipy.misc import imread, imresize\n","from math import floor, ceil     \n","import time\n","import cv2\n","\n","\n","from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import data_to_input, mirror_joints_map, CropImage, DataItem\n","\n","class PoseDataset:\n","    def __init__(self, dlc_cfg):\n","        self.dlc_cfg = dlc_cfg\n","        self.data = self.load_dataset()\n","        self.num_images = len(self.data)\n","        if self.dlc_cfg.mirror:\n","            self.symmetric_joints = mirror_joints_map(dlc_cfg.all_joints, dlc_cfg.num_joints)\n","        self.curr_img = 0\n","        self.set_shuffle(dlc_cfg.shuffle)\n","        # self.successes = np.array([0]*self.num_images)    #debug\n","        # self.required_sizes = {61,62,63,0}    #leg\n","        self.batch_number = 1\n","\n","    def load_dataset(self):\n","        dlc_cfg = self.dlc_cfg\n","        file_name = os.path.join(self.dlc_cfg.project_path,dlc_cfg.dataset)\n","        # Load Scoremaps\n","        # scoremaps = np.load(self.dlc_cfg.project_path + '/batched-data/Processed_Scoremaps' + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)\n","\n","        # Load Matlab file dataset annotation\n","        mlab = sio.loadmat(file_name)\n","        self.raw_data = mlab\n","        mlab = mlab['dataset']\n","\n","        num_images = mlab.shape[1]\n","        data = []\n","        has_gt = True\n","\n","        for i in range(num_images):\n","            sample = mlab[0, i]\n","\n","            item = DataItem()\n","            item.image_id = i\n","            item.im_path = sample[0][0]\n","            item.im_size = sample[1][0]\n","            # item.image = imread(os.path.join(self.dlc_cfg.project_path,item.im_path), mode='RGB')\n","            save_name = str(self.dlc_cfg.project_path / Path(item.im_path).parents[0] / Path(item.im_path).stem)\n","            item.meta_data = np.load(save_name + '.npz', mmap_mode=None, allow_pickle=False, fix_imports=False)\n","\n","            if len(sample) >= 3:\n","                joints = sample[2][0][0]\n","                joint_id = joints[:, 0]\n","                # make sure joint ids are 0-indexed\n","                if joint_id.size != 0:\n","                    assert((joint_id < dlc_cfg.num_joints).any())\n","                joints[:, 0] = joint_id\n","                item.joints = [joints]\n","            else:\n","                has_gt = False\n","            data.append(item)\n","\n","        self.has_gt = has_gt\n","        return data\n","\n","#     def set_test_mode(self, test_mode):\n","#         self.has_gt = not test_mode\n","\n","    def set_shuffle(self, shuffle):\n","        self.shuffle = shuffle\n","        if not shuffle:\n","            assert not self.dlc_cfg.mirror\n","            self.image_indices = np.arange(self.num_images)\n","\n","    # Not vetted JO\n","    def mirror_joint_coords(self, joints, image_width):\n","        # horizontally flip the x-coordinate, keep y unchanged\n","        joints[:, 1] = image_width - joints[:, 1] - 1\n","        return joints\n","\n","    # Not vetted JO\n","    def mirror_joints(self, joints, symmetric_joints, image_width):\n","        # joint ids are 0 indexed\n","        res = np.copy(joints)\n","        res = self.mirror_joint_coords(res, image_width)\n","        # swap the joint_id for a symmetric one\n","        joint_id = joints[:, 0].astype(int)\n","        res[:, 0] = symmetric_joints[joint_id]\n","        return res\n","\n","    def shuffle_images(self):\n","        num_images = self.num_images\n","        if self.dlc_cfg.mirror:\n","            image_indices = np.random.permutation(num_images * 2)\n","            self.mirrored = image_indices >= num_images\n","            image_indices[self.mirrored] = image_indices[self.mirrored] - num_images\n","            self.image_indices = image_indices\n","        else:\n","            self.image_indices = np.random.permutation(num_images)\n","\n","    def num_training_samples(self):\n","        num = self.num_images\n","        if self.dlc_cfg.mirror:\n","            num *= 2\n","        return num\n","\n","    def next_training_sample(self):\n","        if self.curr_img == 0 and self.shuffle:\n","            self.shuffle_images()\n","\n","        curr_img = self.curr_img\n","        if curr_img % 150 == 0:\n","          self.loaded_images = np.load(os.path.join(self.dlc_cfg.project_path,('batched-data/Image_Batch_'+str(self.batch_number)+'.npz')), mmap_mode=None, allow_pickle=False, fix_imports=False)\n","          self.batch_number += 1      #magic number\n","          if self.batch_number == 16:\n","            self.batch_number = 1 \n","\n","        self.curr_img = (self.curr_img + 1) % self.num_training_samples()\n","\n","        imidx = self.image_indices[curr_img]\n","        mirror = self.dlc_cfg.mirror and self.mirrored[curr_img]\n","\n","        return imidx, mirror\n","\n","    def get_training_sample(self, imidx):\n","        return self.data[imidx]\n","\n","    def get_scale(self):\n","        dlc_cfg = self.dlc_cfg\n","        scale = dlc_cfg.global_scale\n","        if hasattr(dlc_cfg, 'scale_jitter_lo') and hasattr(dlc_cfg, 'scale_jitter_up'):\n","            scale_jitter = rand.uniform(dlc_cfg.scale_jitter_lo, dlc_cfg.scale_jitter_up)\n","            scale *= scale_jitter\n","        return scale                                                                                                                                                        \n","\n","    def next_batch(self):\n","        while True:\n","            imidx, mirror = self.next_training_sample()\n","            data_item = self.get_training_sample(imidx)\n","            if data_item.im_size[1]==1080:\n","              scale = 0.5334903964194936\n","            else:\n","              scale = 0.7579494437963867/2\n","\n","            if not self.is_valid_size(data_item.im_size, scale):\n","\n","                continue\n","            \n","            return self.make_batch(data_item, scale, mirror)\n","\n","    def is_valid_size(self, image_size, scale):\n","        im_width = image_size[2]\n","        im_height = image_size[1]\n","        s_im_width = im_width * scale\n","        s_im_height = im_height * scale\n","\n","        max_input_size = 100\n","        if im_height < max_input_size or im_width < max_input_size:\n","            return False\n","\n","        if hasattr(self.dlc_cfg, 'max_input_size'):\n","            max_input_size = self.dlc_cfg.max_input_size\n","            if s_im_width * s_im_height > max_input_size * max_input_size:\n","                return False\n","            \n","        #debug, #leg\n","        # if (floor(s_im_width) % 64) or (floor(s_im_height) % 64) not in self.required_sizes:\n","        #   # Checks that calculated scale works\n","        #   if self.valid_passes == 1:\n","        #     self.valid_passes = 0\n","        #     return False\n","        #   self.valid_passes = 1\n","        #   round_size = (np.round(image_size*scale/64)*64).astype(int) \n","        #   # hsv = round_size[1]/s_im_height\n","        #   # wsv = round_size[2]/s_im_width\n","        #   # print('Debug from valid: scale: {}\\t hsv:{} wsv:{}\\nImg Size: {}\\tround_size: {}\\n'.format(scale,hsv,wsv,image_size,round_size))\n","        #   self.current_scale = scale * float((round_size[2]/s_im_width+round_size[1]/s_im_height)/2)\n","        #   if not self.is_valid_size(image_size):\n","        #     return False\n","\n","        return True\n","\n","    def make_batch(self, data_item, scale, mirror):\n","        # t0 = time.time()\n","        im_file = data_item.im_path\n","        logging.debug('image %s', im_file)\n","        logging.debug('mirror %r', mirror)\n","        \n","        #print(im_file, os.getcwd())\n","        #print(self.dlc_cfg.project_path)\n","        # image = imread(os.path.join(self.dlc_cfg.project_path,im_file), mode='RGB')\n","        image = self.loaded_images[im_file]\n","\n","        # t1 = time.time()\n","\n","        if self.has_gt:\n","            joints = np.copy(data_item.joints)\n","\n","        if self.dlc_cfg.crop: #adapted cropping for DLC     #debug, changed this to false\n","            if np.random.rand()<self.dlc_cfg.cropratio:\n","                #1. get center of joints\n","                j=np.random.randint(np.shape(joints)[1]) #pick a random joint\n","                # draw random crop dimensions & subtract joint points\n","                #print(joints,j,'ahah')\n","                joints,image=CropImage(joints,image,joints[0,j,1],joints[0,j,2],self.dlc_cfg)\n","                \n","                #if self.has_gt:\n","                #    joints[0,:, 1] -= x0\n","                #    joints[0,:, 2] -= y0\n","                '''\n","                print(joints)\n","                import matplotlib.pyplot as plt\n","                plt.clf()\n","                plt.imshow(image)\n","                plt.plot(joints[0,:,1],joints[0,:,2],'.')\n","                plt.savefig(\"abc\"+str(np.random.randint(int(1e6)))+\".png\")\n","                '''\n","            else:\n","                pass #no cropping!\n","\n","        #Debug\n","        # print('Debug from Dataset\\nImage.shape: {}\\tImage type: {}\\tScale: {}\\tScale type: {}\\n'.format(image.shape,type(image),scale,type(scale)))\n","\n","        img = imresize(image, scale) if scale != 1 else image\n","        scaled_img_size = np.array(img.shape[0:2])\n","        # if scale == 0.7579494437963867/2:\n","        #   img = img[:,1:,:]\n","\n","        if mirror:\n","            img = np.fliplr(img)\n","\n","        batch = {Batch.inputs: img}\n","\n","        # t2 = time.time()\n","\n","        #JO Add Scoremap and Locref Loading and Batch assignment\n","        locref = data_item.meta_data['locref']\n","        scmap = data_item.meta_data['scmap']\n","        scmaps_loaded = np.concatenate((locref[:,:,:,0],locref[:,:,:,1],scmap),axis=2) \n","        scmaps_loaded = cv2.resize(scmaps_loaded, dsize=(256, 144), interpolation=cv2.INTER_CUBIC)\n","\n","\n","        #PSE, #cobble \n","        # dim0_increase_needed = ceil((floor((img.shape[0]-1)/2)+1)/2)-scmaps_loaded.shape[0]\n","        # dim1_increase_needed = ceil((floor((img.shape[1]-1)/2)+1)/2)-scmaps_loaded.shape[1]\n","        # print(scmaps_loaded.shape)\n","        # scmaps_loaded = np.pad(scmaps_loaded,[[floor(dim0_increase_needed/2),dim0_increase_needed-floor(dim0_increase_needed/2)],[floor(dim1_increase_needed/2),dim1_increase_needed-floor(dim1_increase_needed/2)],[0,0]],mode='constant')\n","\n","        #debug   \n","        # global t_img  \n","        # global t_sms \n","        # t_img = img\n","        # t_sms = scmaps_loaded\n","\n","        #Debug\n","        # print('Debug from PoseDataset:Im_num: {}\\tO_Image shape: {}\\tImg shape:{}\\tScmaps shape: {}\\tRatio: {}'.format\n","        #       (self.generated, image.shape,img.shape, scmaps_loaded.shape, img.shape[0]/scmaps_loaded.shape[0]))\n","\n","        batch.update({\n","            Batch.scmap_inputs: scmaps_loaded,\n","        })\n","\n","        # t3 = time.time()\n","\n","        if self.has_gt:\n","            stride = self.dlc_cfg.stride\n","\n","            if mirror:\n","                joints = [self.mirror_joints(person_joints, self.symmetric_joints, image.shape[1]) for person_joints in\n","                          joints]\n","\n","            #JO, PSE  #debug, #hack\n","            #sm_size = np.ceil(scaled_img_size / (stride * 2)).astype(int)* 2 \n","            # sm_size = (np.round(scaled_img_size/64)*32).astype(int)     #JO Full Scale Output \n","            sm_size = (np.round(scaled_img_size/64)*32).astype(int)     #JO Half Scale Output \n","            sm_scale =float((sm_size[0]/image.shape[0]+sm_size[1]/image.shape[1])/2)\n","\n","            scaled_joints = [person_joints[:, 1:3] * sm_scale for person_joints in joints] #JO  \n","\n","            joint_id = [person_joints[:, 0].astype(int) for person_joints in joints]\n","            part_score_targets, part_score_weights, locref_targets, locref_mask = self.compute_target_part_scoremap(\n","                joint_id, scaled_joints, data_item, sm_size, sm_scale) #JO\n","\n","            batch.update({\n","                Batch.part_score_targets: part_score_targets,\n","                Batch.part_score_weights: part_score_weights,\n","                Batch.locref_targets: locref_targets,\n","                Batch.locref_mask: locref_mask\n","            })\n","\n","        # t4 = time.time()\n","        #Debug\n","        # print('Debug from PoseDataset:\\nImage shape: {}\\nScmaps shape: {}\\nRatio: {}\\nFailed attempts: {}\\n'.format\n","        #       (img.shape, scmaps_loaded.shape, img.shape[0]/scmaps_loaded.shape[0],self.fails))\n","        # print('Debug from PoseDataset:\\nImage shape: {}\\tImg shape: {}\\tScmaps shape: {}\\tFailed attempts: {}\\t Scale: {}'.format\n","        #       (image.shape,img.shape, scmaps_loaded.shape,self.fails,scale))\n","        # self.fails = 0\n","        # print('pst: {}\\tpsw: {}\\tlt: {}\\tlm: {}\\t sm_scale: {}'.format\n","        #       (part_score_targets.shape,part_score_weights.shape,locref_targets.shape,locref_mask.shape,sm_scale))\n","\n","\n","        batch = {key: np.expand_dims(data, axis=0).astype(float) for (key, data) in batch.items()}\n","\n","        # t5 = time.time()\n","\n","        batch[Batch.data_item] = data_item\n","\n","        # print('t1: {:.4f}\\tt2: {:.4f}\\tt3: {:.4f}\\tt4: {:.4f}\\tt5: {:.4f}\\tt6: {:.4f}'.format(t1-t0,t2-t1,t3-t2,t4-t3,t5-t4,time.time()-t5))  \n","        return batch\n","\n","\n","    #PSE not 100% what this does\n","    def compute_target_part_scoremap(self, joint_id, coords, data_item, size, scale):\n","        stride = 2\n","        dist_thresh = self.dlc_cfg.pos_dist_thresh * scale\n","        num_joints = self.dlc_cfg.num_joints\n","        half_stride = stride / 2 if stride != 1 else 1\n","        scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))\n","        locref_size = np.concatenate([size, np.array([num_joints * 2])])\n","        locref_mask = np.zeros(locref_size)\n","        locref_map = np.zeros(locref_size)\n","\n","        locref_scale = 1.0 / self.dlc_cfg.locref_stdev\n","        dist_thresh_sq = dist_thresh ** 2\n","\n","        width = size[1]\n","        height = size[0]\n","\n","        for person_id in range(len(coords)):\n","            for k, j_id in enumerate(joint_id[person_id]):\n","                joint_pt = coords[person_id][k, :]\n","                j_x = np.asscalar(joint_pt[0])\n","                j_y = np.asscalar(joint_pt[1])\n","\n","                # don't loop over entire heatmap, but just relevant locations\n","                j_x_sm = round((j_x - half_stride) / stride)\n","                j_y_sm = round((j_y - half_stride) / stride)\n","                min_x = round(max(j_x_sm - dist_thresh - 1, 0))\n","                max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))\n","                min_y = round(max(j_y_sm - dist_thresh - 1, 0))\n","                max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))\n","\n","                for j in range(min_y, max_y + 1):  # range(height):\n","                    pt_y = j * stride + half_stride\n","                    for i in range(min_x, max_x + 1):  # range(width):\n","                        # pt = arr([i*stride+half_stride, j*stride+half_stride])\n","                        # diff = joint_pt - pt\n","                        # The code above is too slow in python\n","                        pt_x = i * stride + half_stride\n","                        dx = j_x - pt_x\n","                        dy = j_y - pt_y\n","                        dist = dx ** 2 + dy ** 2\n","                        # print(la.norm(diff))\n","                        if dist <= dist_thresh_sq:\n","                            scmap[j, i, j_id] = 1\n","                            locref_mask[j, i, j_id * 2 + 0] = 1\n","                            locref_mask[j, i, j_id * 2 + 1] = 1\n","                            locref_map[j, i, j_id * 2 + 0] = dx * locref_scale\n","                            locref_map[j, i, j_id * 2 + 1] = dy * locref_scale\n","\n","        weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)\n","\n","        return scmap, weights, locref_map, locref_mask\n","\n","\n","    #PSE not 100% what this does\n","    def compute_scmap_weights(self, scmap_shape, joint_id, data_item):\n","        dlc_cfg = self.dlc_cfg\n","        if dlc_cfg.weigh_only_present_joints:\n","            weights = np.zeros(scmap_shape)\n","            for person_joint_id in joint_id:\n","                for j_id in person_joint_id:\n","                    weights[:, :, j_id] = 1.0\n","        else:\n","            weights = np.ones(scmap_shape)\n","        return weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W1X8FbaKDzVc","colab_type":"text"},"source":["## train"]},{"cell_type":"markdown","metadata":{"id":"Ph3_JQuuPz43","colab_type":"text"},"source":["### train function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CmxEouPtfdq9","colab":{}},"source":["# Adapted from DLC train\n","# def train(config_yaml,displayiters,saveiters,maxiters,max_to_keep=5): #debug\n","# train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep) #debug\n","\n","# Note: ss (set scale) model trained up to 21000 on first step lr\n","# Note: DHG -> Double hour glass\n","\n","%cd /content/cloned-DLC-repo\n","\n","###\n","# Parameter declaration\n","###\n","\n","trainediters = -1\n","maxiters = None\n","displayiters = 1000 \n","saveiters = 1000\n","\n","import time   #debug\n","\n","from deeplabcut.pose_estimation_tensorflow.nnet import losses\n","from deeplabcut.pose_estimation_tensorflow.train import setup_preloading, start_preloading, load_and_enqueue, get_optimizer\n","from deeplabcut.pose_estimation_tensorflow.util.logging import setup_logging\n","\n","os.chdir(path_extension) #switch to folder of config_yaml (for logging)\n","setup_logging()\n","\n","dlc_cfg['batch_size']=1 #in case this was edited for analysis.\n","\n","data_start_time = time.time()\n","print('Starting dataset loading')\n","dataset = PoseDataset(dlc_cfg)\n","print('Dataset loading took: ',time.time()-data_start_time)\n","\n","batch_spec = get_batch_spec(dlc_cfg)\n","batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n","losses = PoseNet(dlc_cfg).train(batch)\n","total_loss = losses['total_loss']\n","\n","for k, t in losses.items():\n","    TF.summary.scalar(k, t)\n","merged_summaries = TF.summary.merge_all()\n","\n","# Need to figure out what to do here if model not trained\n","# Check which snapshots are available and sort them by # iterations\n","found_snapshot = False\n","Snapshots = np.array([fn.split('.')[0]for fn in os.listdir(os.path.join(str(path_extension), ''))if \"index\" in fn])\n","try: #check if any where found?\n","  Snapshots[0]\n","  found_snapshot = True\n","\n","  increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n","  Snapshots = Snapshots[increasing_indices]\n","\n","  if cfg[\"snapshotindex\"] == -1:\n","      snapindex = [-1]\n","  # elif cfg[\"snapshotindex\"] == \"all\":\n","  #     snapindices = range(len(Snapshots))\n","  # elif cfg[\"snapshotindex\"]<len(Snapshots):\n","  #     snapindices=[cfg[\"snapshotindex\"]]\n","  # else:\n","  #     print(\"Invalid choice, only -1 (last), any integer up to last, or all (as string)!\")\n","\n","  dlc_cfg['init_weights'] = os.path.join(str(path_extension),Snapshots[snapindex][0])\n","  trainediters = int((dlc_cfg['init_weights'].split(os.sep)[-1]).split('-')[-1]) #read how many training siterations that corresponds to.\n","\n","  restorer = TF.train.Saver()\n","\n","except IndexError:\n","  print(\"\\nSnapshots not found! It seems the dataset for shuffle %s and trainFraction %s does not exist. Training %s from scratch\\n\"%(shuffle,trainFraction,model_version))\n","\n","saver = TF.train.Saver(max_to_keep=max_snapshots_to_keep) # selects how many snapshots are stored, see https://github.com/AlexEMG/DeepLabCut/issues/8#issuecomment-387404835\n","\n","sess = TF.Session()\n","coord, thread = start_preloading(sess, enqueue_op, dataset, placeholders)\n","train_writer = TF.summary.FileWriter(dlc_cfg.log_dir, sess.graph)\n","learning_rate, train_op = get_optimizer(total_loss, dlc_cfg)\n","\n","sess.run(TF.global_variables_initializer())\n","sess.run(TF.local_variables_initializer())\n","\n","# Restore variables from disk if model has started to be trained\n","if found_snapshot:\n","  restorer.restore(sess, dlc_cfg.init_weights)\n","\n","if maxiters==None:\n","    max_iter = int(dlc_cfg.multi_step[-1][1])\n","else:\n","    max_iter = min(int(dlc_cfg.multi_step[-1][1]),int(maxiters))\n","    #display_iters = max(1,int(displayiters))\n","    print(\"Max_iters overwritten as\",max_iter)\n","\n","if displayiters==None:\n","    display_iters = max(1,int(dlc_cfg.display_iters))\n","else:\n","    display_iters = max(1,int(displayiters))\n","    print(\"Display_iters overwritten as\",display_iters)\n","\n","if saveiters==None:\n","    save_iters=max(1,int(dlc_cfg.save_iters))\n","\n","else:\n","    save_iters=max(1,int(saveiters))\n","    print(\"Save_iters overwritten as\",save_iters)\n","\n","cum_loss = 0.0\n","start_time = time.time()\n","lr_gen = LearningRate(dlc_cfg,trainediters)    \n","\n","### Testing\n","\n","# test_lr = lr_gen.get_lr(0)\n","\n","# print('Debug from Train: Waiting for Data')\n","\n","# for it in range(10):\n","#   wt0 = time.time()\n","#   [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n","#                                     feed_dict={learning_rate: test_lr})\n","#   wt1 = time.time()-wt0\n","#   print('Debug from Training: It: {}\\twt: {}\\tLoss: {}'.format(it,wt1,loss_val))\n","#   # print('Debug from Training: It: {}\\tLoss: {}'.format(it,loss_val))\n","\n","# print('Debug from Train: Image selection list:')\n","# print(dataset.successes[0:1000])\n","# print(dataset.successes[1000:2000])\n","# print(dataset.successes[2000:])\n","\n","### Remaining function\n","stats_path = (Path(path_extension) / 'stats_log').with_name('learning_stats.csv')\n","lrf = open(str(stats_path), 'w')\n","\n","print(\"Training parameter:\")\n","print(dlc_cfg)\n","print(\"Starting training....\")\n","for it in range(trainediters+1,max_iter+1):\n","# for it in range(max_iter+1):\n","    current_lr = lr_gen.get_lr(it)\n","    [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n","                                      feed_dict={learning_rate: current_lr})\n","    cum_loss += loss_val\n","    train_writer.add_summary(summary, it)\n","\n","    if it % display_iters == 0 and it>0:\n","        average_loss = cum_loss / display_iters\n","        average_time = (time.time()-start_time) / display_iters\n","        cum_loss = 0.0\n","        logging.info(\"iteration: {} loss: {} lr: {} s_per_it: {}\"\n","                      .format(it, \"{0:.6f}\".format(average_loss), current_lr, \"{0:.4f}\".format(average_time)))\n","        lrf.write(\"{}, {:.5f}, {}\\n\".format(it, average_loss, current_lr))\n","        lrf.flush()\n","        start_time = time.time()\n","\n","\n","    # Save snapshot\n","    if (it % save_iters == 0 and it != 0) or it == max_iter:\n","        model_name = path_extension +'/' + snapshot_name \n","        saver.save(sess, model_name, global_step=it)\n","\n","lrf.close()\n","sess.close()\n","coord.request_stop()\n","coord.join([thread])\n","\n","os.chdir(str(start_path))    #return to original path.\n","TF.reset_default_graph()"],"execution_count":0,"outputs":[]}]}